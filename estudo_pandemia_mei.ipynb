{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kazzttor/estudomei/blob/main/estudo_pandemia_mei.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQFYzkTpcZW"
      },
      "source": [
        "# Estudo sobre a pandemia e o empreendedorismo por necessidade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1: carregando dados e funcionalidades"
      ],
      "metadata": {
        "id": "pCi3JL-XXiMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas unidecode tqdm fastkml\n",
        "!pip install -U plotly kaleido orca\n",
        "\n",
        "#Windows VS Code\n",
        "# %pip install --upgrade numpy pip plotly tqdm pandas geopandas fastkml nbformat \"kaleido==0.1.*\" orca unidecode shapely matplotlib requests"
      ],
      "metadata": {
        "id": "1vsV_IN-XZbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests, io, json\n",
        "import os\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import time\n",
        "import unicodedata\n",
        "from shapely.geometry import Point, shape\n",
        "from fastkml import kml\n",
        "from shapely.ops import unary_union\n",
        "from fastkml.kml import Placemark, Folder"
      ],
      "metadata": {
        "id": "la1JnElGXWWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_DADOS = '/content/drive/MyDrive/Colab Notebooks/estudo_mei'\n",
        "\n",
        "def baixar_arquivos_zip(url_base):\n",
        "    # Cria a pasta 'dados' se não existir\n",
        "    dir_dados = DIR_DADOS\n",
        "    if not os.path.exists(dir_dados):\n",
        "        os.makedirs(dir_dados)\n",
        "\n",
        "    # Lista de arquivos a serem baixados\n",
        "    arquivos = ['Cnaes.zip', 'Motivos.zip', 'Municipios.zip', 'Simples.zip'] + [f'Estabelecimentos{i}.zip' for i in range(10)]\n",
        "\n",
        "    for arquivo in arquivos:\n",
        "        url = url_base + arquivo  # Concatena a URL base com o nome do arquivo\n",
        "        nome_arquivo = os.path.join(dir_dados, arquivo)  # Salvar na pasta 'dados'\n",
        "        if os.path.exists(nome_arquivo):\n",
        "            print(f\"Arquivo {nome_arquivo} já existe. Pulando download.\")\n",
        "            continue\n",
        "\n",
        "        # Faz o download do arquivo\n",
        "        print(f\"Baixando {nome_arquivo}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 1024  # Tamanho do bloco para o tqdm\n",
        "        with open(nome_arquivo, 'wb') as file:\n",
        "            for data in tqdm(response.iter_content(block_size), total=total_size // block_size, unit='KB', desc=nome_arquivo):\n",
        "                file.write(data)\n",
        "\n",
        "    print(\"Download concluído.\")\n",
        "\n",
        "def carregar_dados_zip():\n",
        "    dir_dados = DIR_DADOS\n",
        "    # Verifica se a pasta 'dados' existe\n",
        "    if not os.path.exists(dir_dados):\n",
        "        raise FileNotFoundError(\"A pasta 'dados' não foi encontrada. Certifique-se de que os arquivos ZIP foram baixados corretamente.\")\n",
        "\n",
        "    arquivos_zip = ['Cnaes.zip', 'Motivos.zip', 'Municipios.zip', 'Simples.zip'] + [f'Estabelecimentos{i}.zip' for i in range(10)]\n",
        "\n",
        "    # Verifica se os arquivos ZIP existem\n",
        "    for arquivo in arquivos_zip:\n",
        "        caminho_arquivo = os.path.join(dir_dados, arquivo)\n",
        "        if not os.path.exists(caminho_arquivo):\n",
        "            raise FileNotFoundError(f\"O arquivo {arquivo} não foi encontrado na pasta 'dados'. Certifique-se de que o download foi concluído com sucesso.\")\n",
        "\n",
        "    chunk_size = 10000\n",
        "\n",
        "    # Carregar CNAEs\n",
        "    print('Carregando CNAES')\n",
        "    cnaes_df = pd.concat([chunk for chunk in pd.read_csv(f'{dir_dados}/Cnaes.zip', sep=';', header=None, encoding='latin1', names=['codigo', 'descricao'], chunksize=chunk_size, compression='zip', on_bad_lines='skip')], ignore_index=True)\n",
        "\n",
        "    # Carregar Motivos\n",
        "    print('Carregando Motivos')\n",
        "    motivos_df = pd.concat([chunk for chunk in pd.read_csv(f'{dir_dados}/Motivos.zip', sep=';', header=None, encoding='latin1', names=['codigo', 'descricao'], chunksize=chunk_size, compression='zip', on_bad_lines='skip')], ignore_index=True)\n",
        "\n",
        "    # Carregar Municípios\n",
        "    print('Carregando Municípios')\n",
        "    municipios_df = pd.concat([chunk for chunk in pd.read_csv(f'{dir_dados}/Municipios.zip', sep=';', header=None, encoding='latin1', names=['codigo', 'nome'], chunksize=chunk_size, compression='zip', on_bad_lines='skip')], ignore_index=True)\n",
        "\n",
        "    # Carregar Simples\n",
        "\n",
        "    simples_chunks = pd.read_csv(f'{dir_dados}/Simples.zip', sep=';', header=None, encoding='latin1',\n",
        "                                 names=['cnpj_basico', 'opcao_simples', 'data_opcao_simples',\n",
        "                                        'data_exclusao_simples', 'opcao_mei', 'data_opcao_mei',\n",
        "                                        'data_exclusao_mei'],\n",
        "                                 chunksize=chunk_size, compression='zip', on_bad_lines='skip')\n",
        "\n",
        "    simples_df = pd.concat([chunk for chunk in tqdm(simples_chunks, desc=\"Carregando Simples\", unit=\"chunk\")], ignore_index=True)\n",
        "\n",
        "    # Carregar Estabelecimentos e mesclar dados\n",
        "    estabelecimentos_df_list = []\n",
        "    for i in range(10):\n",
        "        estabelecimentos_chunks = pd.read_csv(\n",
        "            f'{dir_dados}/Estabelecimentos{i}.zip', sep=';', header=None, encoding='latin1',\n",
        "            names=[\n",
        "                'cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia',\n",
        "                'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_cidade_exterior',\n",
        "                'pais', 'data_inicio', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_logradouro',\n",
        "                'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1',\n",
        "                'ddd_2', 'telefone_2', 'ddd_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_situacao_especial'\n",
        "            ],\n",
        "            chunksize=chunk_size, compression='zip', on_bad_lines='skip'\n",
        "        )\n",
        "\n",
        "        for chunk in tqdm(estabelecimentos_chunks, desc=f'Carregando estabelecimentos {i+1}/10', unit='chunk'):\n",
        "            chunk_filtered = chunk.loc[chunk['identificador_matriz_filial'] == 1, [\n",
        "                'cnpj_basico', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral',\n",
        "                'data_inicio', 'cnae_fiscal_principal', 'logradouro', 'numero', 'bairro', 'uf', 'municipio'\n",
        "            ]]\n",
        "            estabelecimentos_df_list.append(chunk_filtered)\n",
        "\n",
        "    estabelecimentos_df = pd.concat(estabelecimentos_df_list, ignore_index=True)\n",
        "\n",
        "    # Substituir o código de município pelo nome\n",
        "    print('Anexando Municípios')\n",
        "    estabelecimentos_df = estabelecimentos_df.merge(municipios_df, left_on='municipio', right_on='codigo', how='left').drop(columns=['municipio', 'codigo']).rename(columns={'nome': 'municipio_nome'})\n",
        "\n",
        "    # Substituir o código de situação cadastral pelo texto correspondente\n",
        "    print('Anexando códigos de situação cadastral')\n",
        "    situacao_map = {1: 'nula', 2: 'ativa', 3: 'suspensa', 4: 'inapta', 8: 'baixada'}\n",
        "    estabelecimentos_df['situacao_cadastral'] = estabelecimentos_df['situacao_cadastral'].map(situacao_map)\n",
        "\n",
        "    # Substituir o código do motivo da situação cadastral pelo valor correspondente\n",
        "    print('Anexando códigos de motivo da situação cadastral')\n",
        "    estabelecimentos_df = estabelecimentos_df.merge(motivos_df, left_on='motivo_situacao_cadastral', right_on='codigo', how='left').drop(columns=['motivo_situacao_cadastral', 'codigo']).rename(columns={'descricao': 'motivo_situacao_cadastral'})\n",
        "\n",
        "    # Substituir o código do CNAE fiscal principal pelo valor correspondente\n",
        "    print('Anexando CNAES')\n",
        "    estabelecimentos_df = estabelecimentos_df.merge(cnaes_df, left_on='cnae_fiscal_principal', right_on='codigo', how='left').drop(columns=['cnae_fiscal_principal', 'codigo']).rename(columns={'descricao': 'cnae_fiscal_principal'})\n",
        "\n",
        "    # Vincular os dados do Simples pelo CNPJ BÁSICO\n",
        "    print('Anexando dados do Simples')\n",
        "    estabelecimentos_df = estabelecimentos_df.merge(simples_df, on='cnpj_basico', how='left')\n",
        "\n",
        "    # Filtrar apenas os estabelecimentos com MEI (data_opcao_mei não nula)\n",
        "    print('Filtrando MEI\\'s')\n",
        "    print(f'Entrada: {estabelecimentos_df.shape[0]} CNPJ\\'s totais')\n",
        "    estabelecimentos_df = estabelecimentos_df[(estabelecimentos_df['data_opcao_mei'].notna()) & (estabelecimentos_df['data_opcao_mei'] != 0)]\n",
        "    print(f'Saída: {estabelecimentos_df.shape[0]} CNPJ\\'s MEI\\'s em histórico e atuais.')\n",
        "\n",
        "    # Converter colunas de data para datetime no formato AAAAMMDD\n",
        "    print('Padronizando colunas de data')\n",
        "    estabelecimentos_df['data_situacao_cadastral'] = pd.to_datetime(estabelecimentos_df['data_situacao_cadastral'], format='%Y%m%d', errors='coerce')\n",
        "    estabelecimentos_df['data_inicio'] = pd.to_datetime(estabelecimentos_df['data_inicio'], format='%Y%m%d', errors='coerce')\n",
        "    estabelecimentos_df['data_opcao_simples'] = pd.to_datetime(estabelecimentos_df['data_opcao_simples'], format='%Y%m%d', errors='coerce')\n",
        "    estabelecimentos_df['data_exclusao_simples'] = pd.to_datetime(estabelecimentos_df['data_exclusao_simples'], format='%Y%m%d', errors='coerce')\n",
        "    estabelecimentos_df['data_opcao_mei'] = pd.to_datetime(estabelecimentos_df['data_opcao_mei'], format='%Y%m%d', errors='coerce')\n",
        "    estabelecimentos_df['data_exclusao_mei'] = pd.to_datetime(estabelecimentos_df['data_exclusao_mei'], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "    return estabelecimentos_df\n",
        "\n",
        "def backup_dataframe(df_estabelecimentos, output=f'{DIR_DADOS}/estabelecimentos_mei.zip'):\n",
        "    chunk_size = 10000\n",
        "    if not os.path.exists('dados'):\n",
        "        os.makedirs('dados')\n",
        "    raiz = os.path.splitext(output)\n",
        "    csv_file = f'{raiz[0]}.csv'\n",
        "\n",
        "    # Exportar dataframe para CSV em chunks\n",
        "    chunks = np.array_split(df_estabelecimentos.index, chunk_size)\n",
        "    for chunck, subset in enumerate(tqdm(chunks, desc='Exportando dataframe', unit='chunk')):\n",
        "        if chunck == 0:  # primeira linha\n",
        "            df_estabelecimentos.loc[subset].to_csv(csv_file, mode='w', index=True)\n",
        "        else:\n",
        "            df_estabelecimentos.loc[subset].to_csv(csv_file, header=None, mode='a', index=True)\n",
        "    print('Backup do dataframe realizado com sucesso!')\n",
        "\n",
        "    # Compactar o arquivo CSV em um arquivo ZIP\n",
        "    with zipfile.ZipFile(output, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(csv_file, os.path.basename(csv_file))\n",
        "\n",
        "    # Verificar se a compressão foi bem-sucedida e remover o arquivo CSV original\n",
        "    if os.path.exists(output):\n",
        "        os.remove(csv_file)\n",
        "        print(f'Compressão bem-sucedida! Arquivo {csv_file} removido.')\n",
        "    else:\n",
        "        print(f'Compressão falhou! Arquivo {csv_file} não removido.')\n",
        "\n",
        "def restore_dataframe(arquivo, filtros=None):\n",
        "    if not os.path.exists(arquivo):\n",
        "        raise FileNotFoundError(f\"O arquivo {arquivo} não foi encontrado. Certifique-se de que o arquivo exista e que o local do arquivo foi informado corretamente.\")\n",
        "\n",
        "    print(f'Restaurando dataframe a partir do arquivo {arquivo}')\n",
        "    chunk_size = 10000\n",
        "\n",
        "    # Definir os tipos de dados das colunas, exceto as descartadas (índice e número)\n",
        "    dtypes = {\n",
        "        'cnpj_basico': 'Int64',\n",
        "        'latitude': 'float64',\n",
        "        'longitude': 'float64'\n",
        "    }\n",
        "\n",
        "    parse_dates = [\n",
        "        'data_situacao_cadastral',\n",
        "        'data_inicio',\n",
        "        'data_opcao_simples',\n",
        "        'data_exclusao_simples',\n",
        "        'data_opcao_mei',\n",
        "        'data_exclusao_mei'\n",
        "    ]\n",
        "\n",
        "    # Verificar as colunas existentes e descartar as que não serão usadas\n",
        "    first_chunk = pd.read_csv(arquivo, nrows=1, compression='zip')\n",
        "    columns_to_use = [col for col in first_chunk.columns if col not in ['Unnamed: 0', 'numero']]\n",
        "\n",
        "    dtypes = {col: dtype for col, dtype in dtypes.items() if col in columns_to_use}\n",
        "    parse_dates = [col for col in parse_dates if col in columns_to_use]\n",
        "\n",
        "    # Ler o arquivo em chunks, descartando as colunas 'Unnamed: 0' e 'numero'\n",
        "    chunks = pd.read_csv(\n",
        "        arquivo,\n",
        "        chunksize=chunk_size,\n",
        "        compression='zip',\n",
        "        on_bad_lines='skip',\n",
        "        dtype=dtypes,\n",
        "        parse_dates=parse_dates,\n",
        "        usecols=columns_to_use\n",
        "    )\n",
        "\n",
        "    # Filtrar e concatenar os chunks\n",
        "    df_restored = pd.concat(\n",
        "        [chunk.query(filtros) if filtros else chunk for chunk in tqdm(chunks, desc=\"Carregando Dataframe\", unit=\"chunk\")],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    print('Dataframe restaurado com sucesso!')\n",
        "    return df_restored\n",
        "\n",
        "def plotar_mapa_estabelecimentos(df_estabelecimentos, cidade, nome_arquivo_mapa=None, titulo=None, nota=None):\n",
        "    print('Gerando mapa, Aguarde...')\n",
        "    df_com_coordenadas = df_estabelecimentos.dropna(subset=['latitude', 'longitude'])\n",
        "    if titulo is None:\n",
        "        titulo = f\"Mapa de Estabelecimentos MEI em {cidade}\"\n",
        "    if nota is not None:\n",
        "        notarodape = f\"{nota}<br>Fonte: Receita Federal do Brasil e IBGE. Elaborado pelo autor.\"\n",
        "    else:\n",
        "        notarodape = \"Fonte: Receita Federal do Brasil e IBGE. Elaborado pelo autor.\"\n",
        "\n",
        "    gdf_com_coordenadas = gpd.GeoDataFrame(df_com_coordenadas, geometry=gpd.points_from_xy(df_com_coordenadas.longitude, df_com_coordenadas.latitude))\n",
        "    df_agrupado = gdf_com_coordenadas.groupby('bairro')['geometry'].apply(lambda x: x.unary_union.centroid).reset_index()\n",
        "    df_agrupado['latitude'] = df_agrupado['geometry'].apply(lambda x: x.y)\n",
        "    df_agrupado['longitude'] = df_agrupado['geometry'].apply(lambda x: x.x)\n",
        "\n",
        "    total_estabelecimentos_bairro = df_estabelecimentos.groupby('bairro').size().reset_index(name='count')\n",
        "    df_agrupado = pd.merge(df_agrupado, total_estabelecimentos_bairro, on='bairro')\n",
        "\n",
        "    if cidade.lower() == 'são paulo':\n",
        "        df_agrupado = pd.merge(df_agrupado, df_com_coordenadas[['bairro', 'centro_expandido']].drop_duplicates(), on='bairro', how='left')\n",
        "        df_agrupado['color'] = df_agrupado['centro_expandido'].apply(lambda x: 'blue' if x == 'S' else 'red')\n",
        "        df_agrupado['color_label'] = df_agrupado['centro_expandido'].apply(lambda x: 'Centro Expandido' if x == 'S' else 'Periferia')\n",
        "        color_column = 'color_label'\n",
        "    else:\n",
        "        color_column = None\n",
        "\n",
        "    centro_lat = df_com_coordenadas['latitude'].mean()\n",
        "    centro_lon = df_com_coordenadas['longitude'].mean()\n",
        "    geodf = ibge_municipio_geodataframe(ibge_id_municipio(slugify(cidade))[0])\n",
        "    bounds = geodf.total_bounds\n",
        "    centro_municipio = geodf.geometry.centroid.iloc[0]\n",
        "\n",
        "    def calcular_zoom(bounds):\n",
        "        minx, miny, maxx, maxy = bounds\n",
        "        x_diff = maxx - minx\n",
        "        y_diff = maxy - miny\n",
        "        max_diff = max(x_diff, y_diff)\n",
        "        zoom = 12 - (max_diff/0.25)\n",
        "        return zoom\n",
        "\n",
        "    nivel_zoom = calcular_zoom(bounds)\n",
        "\n",
        "    fig = px.scatter_mapbox(df_agrupado,\n",
        "                            lat='latitude',\n",
        "                            lon='longitude',\n",
        "                            size='count',\n",
        "                            hover_name='bairro',\n",
        "                            title=titulo,\n",
        "                            labels={'count': 'Quantidade de Estabelecimentos', 'color_label': 'Região'},\n",
        "                            center={\"lat\": centro_lat, \"lon\": centro_lon},\n",
        "                            color=color_column)\n",
        "\n",
        "    fig.update_layout(\n",
        "        mapbox={\n",
        "            \"style\": \"carto-positron\",\n",
        "            \"zoom\": nivel_zoom,\n",
        "            \"center\":{\"lat\":centro_municipio.y, \"lon\":centro_municipio.x},\n",
        "            \"layers\": [\n",
        "                {\n",
        "                    \"source\": json.loads(geodf.geometry.to_json()),\n",
        "                    \"below\": \"traces\",\n",
        "                    \"type\": \"line\",\n",
        "                    \"color\": \"black\",\n",
        "                    \"line\": {\"width\": 1.5},\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        legend_title_text='Região' if cidade.lower() == 'são paulo' else None,\n",
        "        margin={\"l\": 0, \"r\": 0, \"t\": 50, \"b\": 50},\n",
        "        width=720,\n",
        "        height=800,\n",
        "        annotations=[\n",
        "            dict(\n",
        "                text=notarodape,\n",
        "                x=0, y=0.02,\n",
        "                xref='paper', yref='paper',\n",
        "                showarrow=False,\n",
        "                xanchor='left',\n",
        "                bgcolor='rgba(255, 255, 255, 0.8)'\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if nome_arquivo_mapa:\n",
        "        try:\n",
        "            fig.write_image(f\"{DIR_DADOS}/{nome_arquivo_mapa}.png\")\n",
        "            print(f\"Mapa salvo como '{DIR_DADOS}/{nome_arquivo_mapa}.png'\")\n",
        "        except ValueError as e:\n",
        "            if 'kaleido' in str(e):\n",
        "                print(\"Falha ao utilizar Kaleido, tentando com Orca...\")\n",
        "                fig.write_image(f\"{DIR_DADOS}/{nome_arquivo_mapa}.png\", engine=\"orca\")\n",
        "                print(f\"Mapa salvo como '{DIR_DADOS}/{nome_arquivo_mapa}.png' com Orca\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "def slugify(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "    text = text.replace(' ', '-')\n",
        "    text = ''.join(c for c in text if c.isalnum() or c in ['-', '_'])\n",
        "    return text.lower()\n",
        "\n",
        "def ibge_id_municipio(municipio_slug):\n",
        "    \"\"\"\n",
        "    Busca o ID do município no IBGE usando o slug do município.\n",
        "\n",
        "    Args:\n",
        "        municipio_slug (str): Slug do município (nome formatado para URL).\n",
        "\n",
        "    Returns:\n",
        "        tuple: ID do município e nome do município.\n",
        "    \"\"\"\n",
        "    base_url = f'https://servicodados.ibge.gov.br/api/v1/localidades/municipios/{municipio_slug}'\n",
        "    response = requests.get(base_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Verifica se a resposta contém os dados esperados\n",
        "        if 'id' in data and 'nome' in data:\n",
        "            return data['id'], data['nome']\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de dados inesperado da API: {data}\")\n",
        "    else:\n",
        "        raise Exception(f'Erro ao buscar ID do município: {response.status_code}')\n",
        "\n",
        "def ibge_municipio_geodataframe(id_municipio):\n",
        "    \"\"\"\n",
        "    Busca os dados geográficos do município no IBGE e retorna um GeoDataFrame.\n",
        "\n",
        "    Args:\n",
        "        id_municipio (int): ID do município.\n",
        "\n",
        "    Returns:\n",
        "        GeoDataFrame: Dados geográficos do município.\n",
        "    \"\"\"\n",
        "    url = f'https://servicodados.ibge.gov.br/api/v3/malhas/municipios/{id_municipio}?formato=application/vnd.geo+json'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return gpd.GeoDataFrame.from_features(data['features'])\n",
        "    else:\n",
        "        raise Exception(f'Erro ao obter dados geográficos do município: {response.status_code}')\n",
        "\n",
        "def download_kml(url, dir_dados, filename):\n",
        "  \"\"\"Downloads a KML file from a given URL and saves it to a specified directory.\"\"\"\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    if not os.path.exists(dir_dados):\n",
        "      os.makedirs(dir_dados)\n",
        "\n",
        "    filepath = os.path.join(dir_dados, filename)\n",
        "    with open(filepath, 'wb') as f:\n",
        "      f.write(response.content)\n",
        "\n",
        "    print(f\"KML file downloaded successfully to: {filepath}\")\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading KML file: {e}\")\n",
        "\n",
        "def extrair_bairros_kml(kml_file, df_enderecos):\n",
        "    \"\"\"\n",
        "    Extrai bairros contidos dentro de uma área delimitada por um arquivo KML.\n",
        "\n",
        "    Args:\n",
        "        kml_file (str): Caminho para o arquivo KML.\n",
        "        df_enderecos (pd.DataFrame): DataFrame com dados de endereços e coordenadas geográficas.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame com os bairros contidos na área delimitada.\n",
        "    \"\"\"\n",
        "    # Carregar o arquivo KML como bytes\n",
        "    # with open(kml_file, 'rt') as f: # Change 'rt' to 'rb' to read in binary mode\n",
        "    #     doc = f.read()\n",
        "\n",
        "    # k_data = kml.KML()\n",
        "    # k_data.from_string(doc) # Now 'doc' is a bytes string\n",
        "\n",
        "    k_data = kml.KML.parse(kml_file)\n",
        "\n",
        "    # Extrair a geometria do KML\n",
        "    k_features = k_data.features\n",
        "    placemarks = list(k_features[0].features)\n",
        "    polygons = [shape(placemark.geometry) for placemark in placemarks]\n",
        "\n",
        "    # Converter DataFrame de endereços para GeoDataFrame\n",
        "    gdf_enderecos = gpd.GeoDataFrame(\n",
        "        df_enderecos,\n",
        "        geometry=gpd.points_from_xy(df_enderecos.longitude, df_enderecos.latitude)\n",
        "    )\n",
        "\n",
        "    # Verificar quais pontos estão dentro dos polígonos do KML\n",
        "    bairros_contidos = gdf_enderecos[gdf_enderecos.geometry.apply(lambda x: any(polygon.contains(x) for polygon in polygons))]\n",
        "\n",
        "    return bairros_contidos\n",
        "\n",
        "def logradouros_IBGE(municipio: str, uf: str) -> pd.DataFrame:\n",
        "    nome_arquivo = f'{DIR_DADOS}/df_{slugify(municipio)}_{slugify(uf)}_logradouros.zip'\n",
        "    if os.path.exists(nome_arquivo):\n",
        "        print(f'Restaurando dataframe a partir do arquivo local {nome_arquivo}')\n",
        "        return restore_dataframe(nome_arquivo)\n",
        "    else:\n",
        "        print('Arquivo local não encontrado. Baixando dados do IBGE')\n",
        "        codigos_uf = {\n",
        "            \"AC\": \"12\", \"AL\": \"27\", \"AP\": \"16\", \"AM\": \"13\", \"BA\": \"29\", \"CE\": \"23\", \"DF\": \"53\",\n",
        "            \"ES\": \"32\", \"GO\": \"52\", \"MA\": \"21\", \"MT\": \"51\", \"MS\": \"50\", \"MG\": \"31\", \"PA\": \"15\",\n",
        "            \"PB\": \"25\", \"PR\": \"41\", \"PE\": \"26\", \"PI\": \"22\", \"RJ\": \"33\", \"RN\": \"24\", \"RS\": \"43\",\n",
        "            \"RO\": \"11\", \"RR\": \"14\", \"SC\": \"42\", \"SP\": \"35\", \"SE\": \"28\", \"TO\": \"17\"\n",
        "        }\n",
        "\n",
        "        codigo_uf = codigos_uf.get(uf.upper())\n",
        "        if not codigo_uf:\n",
        "            raise ValueError(\"UF inválida ou não encontrada.\")\n",
        "\n",
        "        municipio_slug = slugify(municipio)\n",
        "        municipio_id, municipio_nome = ibge_id_municipio(municipio_slug)\n",
        "        municipio_nome_formatado = unicodedata.normalize('NFKD', municipio_nome).encode('ascii', 'ignore').decode('utf-8').replace(\" \", \"_\").upper()\n",
        "        url = f\"https://ftp.ibge.gov.br/Cadastro_Nacional_de_Enderecos_para_Fins_Estatisticos/Censo_Demografico_2022/Arquivos_CNEFE/CSV/Municipio/{codigo_uf}_{uf.upper()}/{municipio_id}_{municipio_nome_formatado}.zip\"\n",
        "\n",
        "        tentativas = 3\n",
        "        file_path = f\"{DIR_DADOS}/{municipio_id}_{municipio_nome_formatado}.zip\"\n",
        "\n",
        "        while tentativas > 0:\n",
        "            try:\n",
        "                headers = {\"Range\": f\"bytes={os.path.getsize(file_path)}-\"} if os.path.exists(file_path) else {}\n",
        "                with requests.get(url, headers=headers, stream=True) as response:\n",
        "                    total_size = int(response.headers.get('content-length', 0))\n",
        "                    mode = 'ab' if os.path.exists(file_path) else 'wb'\n",
        "                    with open(file_path, mode) as file, tqdm(\n",
        "                        total=total_size, unit='B', unit_scale=True, desc=f\"Baixando dados de {municipio} - {uf}\"\n",
        "                    ) as pbar:\n",
        "                        for data in response.iter_content(chunk_size=1024):\n",
        "                            file.write(data)\n",
        "                            pbar.update(len(data))\n",
        "                break  # Sucesso, sair do loop\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Erro no download: {e}. Tentando novamente...\")\n",
        "                tentativas -= 1\n",
        "                if tentativas == 0:\n",
        "                    raise Exception(\"Falha ao baixar os dados após 3 tentativas.\")\n",
        "\n",
        "        with open(file_path, 'rb') as file:\n",
        "            with io.BytesIO(file.read()) as file_io:\n",
        "                colunas_selecionadas = ['DSC_LOCALIDADE', 'NOM_SEGLOGR', 'LATITUDE', 'LONGITUDE']\n",
        "                df = pd.read_csv(file_io, compression='zip', sep=';', encoding='latin-1', usecols=colunas_selecionadas).reset_index()\n",
        "                df = df.rename(columns={\n",
        "                    'NOM_SEGLOGR': 'logradouro',\n",
        "                    'DSC_LOCALIDADE': 'bairro',\n",
        "                    'LATITUDE': 'latitude',\n",
        "                    'LONGITUDE': 'longitude'\n",
        "                })\n",
        "                df = df[['logradouro', 'bairro', 'latitude', 'longitude']]\n",
        "                df = df.dropna(subset=['latitude', 'longitude'])\n",
        "                df = df.groupby(['logradouro', 'bairro'])[['latitude', 'longitude']].mean().reset_index()\n",
        "        backup_dataframe(df, output=nome_arquivo)\n",
        "        return df\n",
        "\n",
        "def fase1 ():\n",
        "  # A fase 1 é acionada quando não há nenhum conjunto de dados. Consiste em baixar os dados da receita federal e compilá-los em um dataframe.\n",
        "  url_rfb = 'https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/2024-09/'\n",
        "  baixar_arquivos_zip(url_rfb)\n",
        "  df_estabelecimentos = carregar_dados_zip()\n",
        "  backup_dataframe(df_estabelecimentos)\n",
        "  return df_estabelecimentos\n",
        "\n",
        "def fase2 (arquivo, municipio, uf, modo='df'):\n",
        "  #A fase 2 é acionada quando há os dados de estabelecimentos salvos localmente e compilados para o estudo.\n",
        "  municipio_u = unicodedata.normalize('NFKD', municipio).encode('ascii', 'ignore').decode('utf-8').upper()\n",
        "  uf_u = uf.upper()\n",
        "  if modo == 'df':\n",
        "    df_estabelecimentos = arquivo.loc[(arquivo['municipio_nome'] == municipio_u) & (arquivo['uf'] == uf_u)]\n",
        "  elif modo == 'restore':\n",
        "    df_estabelecimentos = restore_dataframe(arquivo, filtros=f\"municipio_nome == '{municipio_u}' & uf == '{uf_u}'\")\n",
        "  else:\n",
        "    raise ValueError(\"Modo inválido. Use 'df' ou 'restore'.\")\n",
        "  locais_sp = logradouros_IBGE(municipio, uf)\n",
        "  fase2 = pd.merge(df_estabelecimentos, locais_sp, on=['logradouro', 'bairro'], how='inner')\n",
        "  arq_fase2 = f'{DIR_DADOS}/df_{slugify(municipio)}_fase2.zip'\n",
        "  backup_dataframe(fase2, output=arq_fase2)\n",
        "  return fase2\n",
        "\n",
        "def fase3(arq_cidade, cidade, modo=\"df\"):\n",
        "    if modo == \"df\":\n",
        "        df_cidade = arq_cidade\n",
        "    elif modo == \"restore\":\n",
        "        df_cidade = restore_dataframe(arq_cidade)\n",
        "    else:\n",
        "        raise ValueError(\"Modo inválido. Use 'df' ou 'restore'.\")\n",
        "\n",
        "    if cidade == 'São Paulo':\n",
        "        print(\"Rotina especial para a cidade de São Paulo\")\n",
        "        df_com_coordenadas = df_cidade.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "        # Criar GeoDataFrame a partir do DataFrame original\n",
        "        geometry = [Point(xy) for xy in zip(df_com_coordenadas.longitude, df_com_coordenadas.latitude)]\n",
        "        gdf = gpd.GeoDataFrame(df_com_coordenadas, geometry=geometry)\n",
        "\n",
        "        # Agrupar por bairro e calcular o centroide\n",
        "        gdf_centroides = gdf.groupby('bairro').geometry.apply(lambda x: x.union_all().centroid).reset_index()\n",
        "        gdf_centroides.columns = ['bairro', 'centroide']\n",
        "\n",
        "        # Separar latitude e longitude do centroide\n",
        "        gdf_centroides['latitude'] = gdf_centroides.centroide.apply(lambda p: p.y)\n",
        "        gdf_centroides['longitude'] = gdf_centroides.centroide.apply(lambda p: p.x)\n",
        "        gdf_centroides = gdf_centroides.drop(columns='centroide')\n",
        "\n",
        "        bairros_contidos = extrair_bairros_kml(f'{DIR_DADOS}/rodizio_sp.kml', gdf_centroides)\n",
        "        df_cidade['centro_expandido'] = 'N'  # Inicializa a coluna com 'N'\n",
        "        df_cidade.loc[df_cidade['bairro'].isin(bairros_contidos['bairro']), 'centro_expandido'] = 'S'\n",
        "\n",
        "    df_cidade['periodo'] = 'outro'\n",
        "    df_cidade.loc[(df_cidade['data_inicio'] >= '2018-02-03') & (df_cidade['data_inicio'] <= '2020-02-02'), 'periodo'] = 'antes'\n",
        "    df_cidade.loc[(df_cidade['data_inicio'] >= '2020-02-03') & (df_cidade['data_inicio'] <= '2022-05-22'), 'periodo'] = 'pandemia'\n",
        "    df_cidade.loc[(df_cidade['data_inicio'] >= '2022-05-23') & (df_cidade['data_inicio'] <= '2024-05-22'), 'periodo'] = 'depois'\n",
        "    df_cidade = df_cidade[df_cidade['periodo'] != 'outro']\n",
        "\n",
        "    arq_fase3 = f'{DIR_DADOS}/df_{slugify(cidade)}_fase3.zip'\n",
        "    backup_dataframe(df_cidade, output=arq_fase3)\n",
        "    return df_cidade\n",
        "\n",
        "def gerar_df_cidade(cidade, uf):\n",
        "    arquivo_df = f'{DIR_DADOS}/estabelecimentos_mei.zip'\n",
        "    arqfase2 = f'{DIR_DADOS}/df_{slugify(cidade)}_fase2.zip'\n",
        "    arqfase3 = f'{DIR_DADOS}/df_{slugify(cidade)}_fase3.zip'\n",
        "    fase2_df = pd.DataFrame()\n",
        "    fase3_df = pd.DataFrame()\n",
        "    if os.path.exists(arqfase3):\n",
        "        print(f'Fase 3 de {cidade} encontrada em {arqfase3}')\n",
        "        fase3_df = restore_dataframe(arqfase3)\n",
        "        print(f'Total de registros (fase 3): {fase3_df.shape[0]}')\n",
        "    else:\n",
        "        if os.path.exists(arqfase2):\n",
        "            print(f'Fase 3 de {cidade} não encontrada, mas encontrada Fase 2 em {arqfase2}')\n",
        "            fase3_df = fase3(arqfase2, cidade, modo='restore')\n",
        "            print(f'Total de registros (fase 3): {fase3_df.shape[0]}')\n",
        "        else:\n",
        "            if os.path.exists(arquivo_df):\n",
        "                print(f'Fase 3 e 2 de {cidade} não encontradas, mas encontrada Fase 1 em {arquivo_df}')\n",
        "                fase2_df = fase2(arquivo_df, cidade, uf, modo='restore')\n",
        "                print(f'Total de registros (fase 2): {fase2_df.shape[0]}')\n",
        "                fase3_df = fase3(fase2_df, cidade)\n",
        "                print(f'Total de registros (fase 3): {fase3_df.shape[0]}')\n",
        "            else:\n",
        "                print(f'Não concontrados arquivos de dados, gerando dataframes para {cidade}')\n",
        "                df_estabelecimentos = fase1()\n",
        "                print(f'Total de registros (fase 1): {df_estabelecimentos.shape[0]}')\n",
        "                fase2_df = fase2(df_estabelecimentos, cidade, uf)\n",
        "                print(f'Total de registros (fase 2): {fase2_df.shape[0]}')\n",
        "                fase3_df = fase3(fase2_df, cidade)\n",
        "                print(f'Total de registros (fase 3): {fase3_df.shape[0]}')\n",
        "    return fase3_df"
      ],
      "metadata": {
        "id": "p0i5euDMXjKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase 2: carregar dataframes do estudo"
      ],
      "metadata": {
        "id": "yrRg3R6TX_Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_df_diadema():\n",
        "  cidade = 'Diadema'\n",
        "  uf = 'SP'\n",
        "  df_diadema = gerar_df_cidade(cidade, uf)\n",
        "  df_diadema.info()\n",
        "  return df_diadema"
      ],
      "metadata": {
        "id": "gKjBSfXEB5Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_df_saopaulo():\n",
        "  kml_url = \"https://www.google.com/maps/d/kml?mid=1Xg6ACUNSiuS0IOr7p7t9c6IvLx4&resourcekey&lid=z9TqTcegPvdk.kkQKI570N-uE&forcekml=1\"\n",
        "  dir_dados = DIR_DADOS\n",
        "  filename = \"rodizio_sp.kml\"\n",
        "\n",
        "  download_kml(kml_url, dir_dados, filename)\n",
        "\n",
        "  cidade = 'São Paulo'\n",
        "  uf = 'SP'\n",
        "  df_sp = gerar_df_cidade(cidade, uf)\n",
        "  df_sp.info()\n",
        "  return df_sp"
      ],
      "metadata": {
        "id": "BY1-H7QyD-xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_df_bsb():\n",
        "  cidade = 'Brasília'\n",
        "  uf = 'DF'\n",
        "  df_bsb = gerar_df_cidade(cidade, uf)\n",
        "  df_bsb.info()\n",
        "  return df_bsb"
      ],
      "metadata": {
        "id": "x7ggOKsCGSbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_df_rjo():\n",
        "  cidade = 'Rio de Janeiro'\n",
        "  uf = 'RJ'\n",
        "  df_rjo = gerar_df_cidade(cidade, uf)\n",
        "  df_rjo.info()\n",
        "  return df_rjo"
      ],
      "metadata": {
        "id": "K4BZCmlpoj3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase 3: estudo sobre a cidade de São Paulo\n",
        "\n",
        "### Datas marco\n",
        "\n",
        "Emergência sanitária no Brasil:\n",
        "\n",
        "- **Início**: 03/02/2020\n",
        "- **Fim**: 22/05/2022\n",
        "\n",
        "Observação: 03/02/2018 a 02/02/2020 (antes) e 23/05/2022 a 22/05/2024 (depois)"
      ],
      "metadata": {
        "id": "I60MxnQUGGPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes:\n",
        "\n",
        "- Centro expandido: 44749 (25,9 %)\n",
        "- Fora do Centro: 127539 (74,1 %)\n",
        "\n",
        "Pandemia:\n",
        "\n",
        "- Centro expandido: 44981 (20,7 %)\n",
        "- Fora do Centro: 181697 (79,3 %)\n",
        "\n",
        "Depois:\n",
        "\n",
        "- Centro expandido: 35431 (19,2 %)\n",
        "- Fora do Centro: 148329 (80,8 %)\n"
      ],
      "metadata": {
        "id": "jxm4ZPrTKS1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = gerar_df_saopaulo()"
      ],
      "metadata": {
        "id": "yoHPl5tg_hTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged['centro_expandido'].value_counts()"
      ],
      "metadata": {
        "id": "iP6ZdB6-Y63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plotar_mapa_estabelecimentos(df_merged, 'São Paulo',nome_arquivo_mapa='mapa_mei_sp', nota='Estabelecimentos MEI criados na cidade de São Paulo entre 03/02/2018 e 22/05/2024.')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-x5TSl91oO3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análise exploratória São Paulo"
      ],
      "metadata": {
        "id": "741cdBHOZueo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[df_merged['periodo']=='pandemia'][['centro_expandido','situacao_cadastral']].value_counts()"
      ],
      "metadata": {
        "id": "id_GJvRFKjzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['situacao_cadastral']=='ativa')][['centro_expandido','opcao_mei']].value_counts()"
      ],
      "metadata": {
        "id": "_shzccr6_e0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='S') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "_43jpRZBAf6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='N') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "dof_s01ZA8Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='S') & (df_merged['opcao_mei']=='S') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "JaACbjBeBKeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='N') & (df_merged['opcao_mei']=='S') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "E1Fg9FsJBUvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='N') & (df_merged['opcao_mei']=='N') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "f8326mSGBb1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='S') & (df_merged['opcao_mei']=='N') & (df_merged['situacao_cadastral']=='ativa')]['cnae_fiscal_principal'].value_counts()"
      ],
      "metadata": {
        "id": "re7gaUa8BoaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='S')][['situacao_cadastral','motivo_situacao_cadastral']].value_counts()"
      ],
      "metadata": {
        "id": "99L73Z2GwRpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['centro_expandido']=='N')][['situacao_cadastral','motivo_situacao_cadastral']].value_counts()"
      ],
      "metadata": {
        "id": "G5TU4Bd_wqLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.loc[(df_merged['periodo']=='pandemia') & (df_merged['situacao_cadastral']=='baixada')][['motivo_situacao_cadastral','cnae_fiscal_principal']].value_counts()"
      ],
      "metadata": {
        "id": "Cl3eNSWlrYjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráficos Temporais"
      ],
      "metadata": {
        "id": "RgyFa8iqZ8x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Converta a coluna 'data_inicio' para o tipo datetime\n",
        "df_merged['data_inicio'] = pd.to_datetime(df_merged['data_inicio'])\n",
        "\n",
        "# Agrupe os dados mensalmente pela soma de registros, divididos pela coluna 'centro_expandido'\n",
        "df_agrupado = df_merged.groupby([pd.Grouper(key='data_inicio', freq='ME'), 'centro_expandido']).size().reset_index(name='total_registros')\n",
        "\n",
        "# Crie um gráfico de linha mensal\n",
        "fig = px.line(df_agrupado,\n",
        "              x='data_inicio',\n",
        "              y='total_registros',\n",
        "              color='centro_expandido',\n",
        "              title='Número de Registros por Mês e Centro Expandido',\n",
        "              labels={'total_registros': 'Número de Registros', 'data_inicio': 'Mês', 'centro_expandido': 'Localização'},\n",
        "              color_discrete_map={'S': 'blue', 'N': 'red'})\n",
        "\n",
        "# Atualize a legenda\n",
        "fig.for_each_trace(lambda t: t.update(name={'S': 'Centro Expandido', 'N': 'Periferia'}[t.name]))\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "# Exiba o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "JwP1x1DsUeBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Vamos fazer o detalhamento do gráfico anterior, em vez do saldo, vamos discriminar o total de cada situação cadastral, por localização (centro expandido e periferia) em dois gráficos de linha, um pra centro expandido e outro e outro para pefiferia.\n",
        "\n",
        "# Converta a coluna 'data_situacao_cadastral' para o tipo datetime\n",
        "df_merged['data_situacao_cadastral'] = pd.to_datetime(df_merged['data_situacao_cadastral'])\n",
        "\n",
        "# Agrupe os dados mensalmente pelo número de registros por cada situação cadastral, divididos pela coluna 'centro_expandido'\n",
        "df_agrupado = df_merged.groupby([pd.Grouper(key='data_situacao_cadastral', freq='ME'), 'centro_expandido', 'situacao_cadastral']).size().reset_index(name='total_registros')\n",
        "\n",
        "# Crie um gráfico de linha mensal para o Centro Expandido\n",
        "df_centro_expandido = df_agrupado[df_agrupado['centro_expandido'] == 'S']\n",
        "fig2_centro_expandido = px.line(df_centro_expandido,\n",
        "                             x='data_situacao_cadastral',\n",
        "                             y='total_registros',\n",
        "                             color='situacao_cadastral',\n",
        "                             title='Total de Situações Cadastrais por Mês (Centro Expandido)',\n",
        "                             labels={'total_registros': 'Número de Registros', 'data_situacao_cadastral': 'Mês', 'situacao_cadastral': 'Situação Cadastral'})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig2_centro_expandido.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "                            fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                            annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_centro_expandido.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "                            fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                            annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_centro_expandido.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "                            fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                            annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_centro_expandido.show()\n",
        "\n",
        "\n",
        "# Crie um gráfico de linha mensal para a Periferia\n",
        "df_periferia = df_agrupado[df_agrupado['centro_expandido'] == 'N']\n",
        "fig2_periferia = px.line(df_periferia,\n",
        "                        x='data_situacao_cadastral',\n",
        "                        y='total_registros',\n",
        "                        color='situacao_cadastral',\n",
        "                        title='Total de Situações Cadastrais por Mês (Periferia)',\n",
        "                        labels={'total_registros': 'Número de Registros', 'data_situacao_cadastral': 'Mês', 'situacao_cadastral': 'Situação Cadastral'})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig2_periferia.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "                        fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_periferia.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "                        fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_periferia.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "                        fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig2_periferia.show()"
      ],
      "metadata": {
        "id": "rpB75lc-1IyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Vamos fazer o mesmo gráfico da célula anterior, porém em vez do saldo mensal, ser o saldo mensal acumulado.\n",
        "\n",
        "# Converta a coluna 'data_situacao_cadastral' para o tipo datetime\n",
        "df_merged['data_situacao_cadastral'] = pd.to_datetime(df_merged['data_situacao_cadastral'])\n",
        "\n",
        "# Crie uma nova coluna 'saldo_mei_ativos' que representa a diferença entre o número de MEIs ativos e os demais\n",
        "df_merged['saldo_mei_ativos'] = 0  # Inicializa a coluna com 0\n",
        "df_merged.loc[df_merged['situacao_cadastral'] == 'ativa', 'saldo_mei_ativos'] = 1  # Se a situação cadastral é 'ativa', o saldo é 1\n",
        "df_merged.loc[df_merged['situacao_cadastral'] != 'ativa', 'saldo_mei_ativos'] = -1  # Se não, o saldo é -1\n",
        "\n",
        "# Agrupe os dados mensalmente pela soma de registros, divididos pela coluna 'centro_expandido'\n",
        "df_agrupado = df_merged.groupby([pd.Grouper(key='data_situacao_cadastral', freq='ME'), 'centro_expandido'])['saldo_mei_ativos'].sum().reset_index(name='saldo_mei_ativos')\n",
        "\n",
        "# Calcule o saldo acumulado\n",
        "df_agrupado['saldo_acumulado'] = df_agrupado.groupby('centro_expandido')['saldo_mei_ativos'].cumsum()\n",
        "\n",
        "# Crie um gráfico de linha mensal com o saldo acumulado\n",
        "fig = px.area(df_agrupado,\n",
        "              x='data_situacao_cadastral',\n",
        "              y='saldo_acumulado',\n",
        "              color='centro_expandido',\n",
        "              title='Saldo Acumulado de MEIs Ativos por Mês e Centro Expandido',\n",
        "              labels={'saldo_acumulado': 'Saldo Acumulado de MEIs Ativos', 'data_situacao_cadastral': 'Mês', 'centro_expandido': 'Localização'},\n",
        "              color_discrete_map={'S': 'blue', 'N': 'red'})\n",
        "\n",
        "# Atualize a legenda\n",
        "fig.for_each_trace(lambda t: t.update(name={'S': 'Centro Expandido', 'N': 'Periferia'}[t.name]))\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "# Exiba o gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "pz9cRsSJVyGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly-orca"
      ],
      "metadata": {
        "id": "E-wjSGKQBHks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: a partir do dataframe df_sp, faça um gráfico de linha usando a da coluna data_inicio agrupado por mês, bairro, se o bairro é do centro expandido, ou não, e a quantidade de MEI's criados. use o fundo para separar os períodos, antes, pandemia e depois. Separe os bairros do centro expandido como linhas de tons azuis e fora como linhas de tons vermelhos.\n",
        "df_sp = df_merged.copy()\n",
        "# Converta a coluna 'data_inicio' para o tipo datetime\n",
        "df_sp['data_inicio'] = pd.to_datetime(df_sp['data_inicio'])\n",
        "\n",
        "# Agrupe os dados por mês, bairro, centro expandido e conte a quantidade de MEIs\n",
        "df_agrupado = df_sp.groupby([pd.Grouper(key='data_inicio', freq='ME'), 'bairro', 'centro_expandido'])['cnpj_basico'].count().reset_index(name='total_mei')\n",
        "\n",
        "# Crie um gráfico de linha\n",
        "fig = px.line(df_agrupado,\n",
        "              x='data_inicio',\n",
        "              y='total_mei',\n",
        "              color='bairro',\n",
        "              facet_row='centro_expandido',\n",
        "              title='Quantidade de MEIs criados por mês, bairro e localização (Centro Expandido/Fora)',\n",
        "              labels={'data_inicio': 'Mês', 'total_mei': 'Quantidade de MEIs', 'bairro': 'Bairro', 'centro_expandido': 'Centro Expandido'},\n",
        "              color_discrete_map={'S': ['blue', 'royalblue', 'darkblue'],\n",
        "                                 'N': ['red', 'firebrick', 'maroon']})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TnZn2afon3jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: vamos criar uma função que gere a série temporal a partir de um dataframe, conforme modelo já testado, usando como variáveis o dataframe, a coluna temporal de referência, o valor a ser contado, as categorias (as linhas que seriam mensuradas), e plote o gráfico de linha, com as separações de épocas (antes, pandemia e depois)\n",
        "\n",
        "def gerar_serie_temporal_e_plotar(df, coluna_temporal, valor_a_contar, categorias, titulo_grafico):\n",
        "  \"\"\"\n",
        "  Gera uma série temporal a partir de um DataFrame, agrupando por mês e categoria,\n",
        "  e plota um gráfico de linha com as separações de épocas (antes, pandemia, depois).\n",
        "\n",
        "  Args:\n",
        "      df: DataFrame com os dados.\n",
        "      coluna_temporal: Nome da coluna com a data a ser usada na série temporal.\n",
        "      valor_a_contar: Nome da coluna com o valor a ser contado.\n",
        "      categorias: Lista de nomes de colunas para agrupar os dados.\n",
        "      titulo_grafico: Título do gráfico.\n",
        "  \"\"\"\n",
        "\n",
        "  # Converter a coluna temporal para datetime\n",
        "  df[coluna_temporal] = pd.to_datetime(df[coluna_temporal])\n",
        "\n",
        "  # Agrupar os dados por mês e categorias, e contar o valor a ser contado\n",
        "  df_agrupado = df.groupby([pd.Grouper(key=coluna_temporal, freq='M')] + categorias)[valor_a_contar].count().reset_index()\n",
        "\n",
        "  # Criar o gráfico de linha\n",
        "  fig = px.line(df_agrupado,\n",
        "                x=coluna_temporal,\n",
        "                y=valor_a_contar,\n",
        "                color=categorias[0] if categorias else None,\n",
        "                title=titulo_grafico,\n",
        "                labels={valor_a_contar: 'Quantidade', coluna_temporal: 'Mês'})\n",
        "\n",
        "  # Adicionar áreas de fundo para as épocas\n",
        "  fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "               fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "               annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "  fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "               fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "               annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "  fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "               fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "               annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "xvl8xmevXaoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nível de emprego IBGE"
      ],
      "metadata": {
        "id": "1-GtlHEucrkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# URL da API do IBGE\n",
        "url = \"https://servicodados.ibge.gov.br/api/v3/agregados/6379/periodos/201802|201803|201804|201805|201806|201807|201808|201809|201810|201811|201812|201901|201902|201903|201904|201905|201906|201907|201908|201909|201910|201911|201912|202001|202002|202003|202004|202005|202006|202007|202008|202009|202010|202011|202012|202101|202102|202103|202104|202105|202106|202107|202108|202109|202110|202111|202112|202201|202202|202203|202204|202205|202206|202207|202208|202209|202210|202211|202212|202301|202302|202303|202304|202305|202306|202307|202308|202309|202310|202311|202312|202401|202402|202403|202404|202405/variaveis/4097?localidades=N1[all]\"\n",
        "\n",
        "# Fazer a requisição para a API\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Extração dos dados\n",
        "serie_temporal = data[0]['resultados'][0]['series'][0]['serie']\n",
        "\n",
        "# Transformação dos dados em um DataFrame\n",
        "df = pd.DataFrame(serie_temporal.items(), columns=['mesano', 'nivel_ocupacao'])\n",
        "\n",
        "# Conversão da coluna 'mesano' para o formato de data YYYYMM\n",
        "df['mesano'] = pd.to_datetime(df['mesano'], format='%Y%m')\n",
        "df['nivel_ocupacao'] = df['nivel_ocupacao'].astype(float).round(1)\n",
        "# Exibir o DataFrame resultante\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "XtYUnZDKk7tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Criar o gráfico\n",
        "fig = px.line(df,\n",
        "              x='mesano',\n",
        "              y='nivel_ocupacao',\n",
        "              title='Nível de ocupação do Brasil*',\n",
        "              labels={'nivel_ocupacao': 'Nível de ocupação (%)', 'mesano': 'Mês'})\n",
        "\n",
        "# Adicionar áreas de fundo para as épocas\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "# Adicionar uma anotação no rodapé do gráfico\n",
        "fig.add_annotation(\n",
        "    text='Percentual de pessoas com 14 anos ou mais ocupadas. Fonte: IBGE.',\n",
        "    xref='paper', yref='paper',\n",
        "    x=0, y=-0.15,\n",
        "    showarrow=False,\n",
        "    font=dict(size=12)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Lm68X8n-tGr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supondo que df já esteja definido\n",
        "df['mesano'] = pd.to_datetime(df['mesano'], format='%Y%m')\n",
        "\n",
        "# Agregar os dados por ano\n",
        "df['ano'] = df['mesano'].dt.year\n",
        "df_ano_a_ano = df.groupby('ano')['nivel_ocupacao'].mean().reset_index()\n",
        "\n",
        "# Exibir a tabela resultante\n",
        "print(df_ano_a_ano)\n"
      ],
      "metadata": {
        "id": "JESMR-sgv9qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dados de tempo de sobrevivência e inaptidão de MEI's"
      ],
      "metadata": {
        "id": "-LCarWH_c5_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie para o dataset df_sp uma coluna chamada meses_duracao para registros com a situacao_cadastral = baixada como a diferença em meses entre data_situacao_cadastral e data_inicio em meses, podemdo ser fracionado com uma cada decimal. Mantenha os registros não afetados como células nulas.\n",
        "\n",
        "# Crie a coluna 'meses_duracao' com valores nulos\n",
        "df_sp['meses_duracao'] = np.nan\n",
        "\n",
        "# Calcule a diferença em meses apenas para registros com 'situacao_cadastral' igual a 'baixada'\n",
        "baixadas = df_sp['situacao_cadastral'] == 'baixada'\n",
        "df_sp.loc[baixadas, 'meses_duracao'] = (df_sp.loc[baixadas, 'data_situacao_cadastral'] - df_sp.loc[baixadas, 'data_inicio']) / pd.Timedelta(days=30.44)"
      ],
      "metadata": {
        "id": "FqLVanVHhXJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie para o dataset df_sp uma coluna chamada meses_abandono para registros com a situacao_cadastral = inapta como a diferença em meses entre data_situacao_cadastral e data_inicio em meses, podemdo ser fracionado com uma cada decimal. Mantenha os registros não afetados como células nulas.\n",
        "\n",
        "# Crie a coluna 'meses_abandono' com valores nulos\n",
        "df_sp['meses_abandono'] = np.nan\n",
        "\n",
        "# Calcule a diferença em meses apenas para registros com 'situacao_cadastral' igual a 'inapta'\n",
        "inaptas = df_sp['situacao_cadastral'] == 'inapta'\n",
        "df_sp.loc[inaptas, 'meses_abandono'] = (df_sp.loc[inaptas, 'data_situacao_cadastral'] - df_sp.loc[inaptas, 'data_inicio']) / pd.Timedelta(days=30.44)"
      ],
      "metadata": {
        "id": "fohPfl1wmZTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcule o valor médio de meses_duracao e de meses_abandono  e quantidade de registros com meses_duracao e meses_abandono para valores não-nulos do dataframe df_sp agrupados pelas colunas periodo e centro_expandido. Calcule o percentual de registros em relação ao total de registros por grupo (com valores meses_duracao, meses_abandono e nulos)\n",
        "\n",
        "# Group data by 'periodo' and 'centro_expandido'\n",
        "grouped = df_sp.groupby(['periodo', 'centro_expandido'])\n",
        "\n",
        "# Calculate the mean of 'meses_duracao' and 'meses_abandono', and the count of non-null values\n",
        "result = grouped[['meses_duracao', 'meses_abandono']].agg(['mean', 'count'])\n",
        "\n",
        "# Rename columns for clarity\n",
        "result.columns = ['tempo_medio_baixa', 'empresas_baixadas', 'tempo_medio_inaptidao', 'empresas_inaptas']\n",
        "\n",
        "# Calculate the total count of records for each group (including null values)\n",
        "total_counts = grouped.size().rename('total_registros')\n",
        "\n",
        "# Combine the results with the total counts\n",
        "result = pd.concat([result, total_counts], axis=1)\n",
        "\n",
        "# Calculate the percentage of records\n",
        "result['percentual_baixadas'] = (result['empresas_baixadas'] / result['total_registros']) * 100\n",
        "result['percentual_inaptas'] = (result['empresas_inaptas'] / result['total_registros']) * 100\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "jEoiBJn5HTzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie uma coluna chamada desenquadrado com valores S ou N onde o valor de  data_exclusao_mei seja não-nulo e maior que data_opcao_mei para o dataframe df_sp\n",
        "\n",
        "# Crie a coluna 'desenquadrado' com valores 'N' inicialmente\n",
        "df_sp['desenquadrado'] = 'N'\n",
        "\n",
        "# Atualize os valores para 'S' onde a condição é satisfeita\n",
        "df_sp.loc[(df_sp['data_exclusao_mei'].notnull()) & (df_sp['data_exclusao_mei'] > df_sp['data_opcao_mei']), 'desenquadrado'] = 'S'"
      ],
      "metadata": {
        "id": "3E1rFHH4YnCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie uma coluna chamada status  para o dataframe df_sp onde caso a sutuação cadastral esteja baixada, o status deve estar como encerrada, caso esteja ativa verifique se a coluna desenquadrado está com o valor S, e caso positivo, atribua o valor desenquadrado, e caso contrário como ativo. para as demais situações cadastrais atribuir o valor empresa irregular\n",
        "\n",
        "# Crie a coluna 'status' com valores iniciais como 'empresa irregular'\n",
        "df_sp['status'] = 'empresa irregular'\n",
        "\n",
        "# Atualize o status com base nas condições especificadas\n",
        "df_sp.loc[df_sp['situacao_cadastral'] == 'baixada', 'status'] = 'encerrada'\n",
        "df_sp.loc[(df_sp['situacao_cadastral'] == 'ativa') & (df_sp['desenquadrado'] == 'S'), 'status'] = 'desenquadrado'\n",
        "df_sp.loc[(df_sp['situacao_cadastral'] == 'ativa') & (df_sp['desenquadrado'] == 'N'), 'status'] = 'ativo'"
      ],
      "metadata": {
        "id": "PdDeFVqGg0hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sp[['periodo','centro_expandido','status']].value_counts()"
      ],
      "metadata": {
        "id": "Lr1GuuVBg8zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Plote uma tabela usando essa informação df_sp[['periodo','centro_expandido','status']].value_counts() como base, mas usando os valores de período e centro_expandido como nomes de colunas. Troque N para Periferia e S para Centro Expandido na coluna centro_expandido.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_sp is already defined as in the provided code\n",
        "\n",
        "# Create the table\n",
        "table = df_sp[['periodo', 'centro_expandido', 'status']].value_counts().reset_index(name='count')\n",
        "\n",
        "# Replace 'N' and 'S' in 'centro_expandido'\n",
        "table['centro_expandido'] = table['centro_expandido'].replace({'N': 'Periferia', 'S': 'Centro Expandido'})\n",
        "\n",
        "# Pivot the table\n",
        "pivot_table = pd.pivot_table(table, values='count', index=['periodo', 'status'], columns='centro_expandido', aggfunc='sum', fill_value=0)\n",
        "\n",
        "# Display the table\n",
        "pivot_table"
      ],
      "metadata": {
        "id": "XPpU9Zd0h9Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Supondo que df_sp já esteja definido\n",
        "\n",
        "# Criar a tabela de contagem\n",
        "table = df_sp[['periodo', 'centro_expandido', 'status']].value_counts().reset_index(name='count')\n",
        "\n",
        "# Substituir 'N' e 'S' em 'centro_expandido'\n",
        "table['centro_expandido'] = table['centro_expandido'].replace({'N': 'Periferia', 'S': 'Centro Expandido'})\n",
        "\n",
        "# Definir a ordem correta dos períodos\n",
        "periodos_esperados = ['antes', 'pandemia', 'depois']\n",
        "table['periodo'] = pd.Categorical(table['periodo'], categories=periodos_esperados, ordered=True)\n",
        "\n",
        "# Pivotar a tabela\n",
        "pivot_table = pd.pivot_table(\n",
        "    table,\n",
        "    values='count',\n",
        "    index=['centro_expandido', 'periodo'],\n",
        "    columns=['status'],\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "# Definir os cabeçalhos\n",
        "headers = ['Região', 'Período', 'Ativo', 'Desenquadrado', 'Empresa Irregular', 'Encerrada']\n",
        "\n",
        "# Preparar os valores das células\n",
        "cells_values = [pivot_table[col].tolist() for col in pivot_table.columns]\n",
        "\n",
        "# Criar a tabela no Plotly\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(\n",
        "        values=headers,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill_color='lightblue',\n",
        "        font=dict(color='black', size=12),\n",
        "        height=30\n",
        "    ),\n",
        "    cells=dict(\n",
        "        values=cells_values,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill=dict(color=['white', 'lightgray']),\n",
        "        font=dict(color='black', size=11),\n",
        "        height=25\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "pCuVlmp0myhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Supondo que df_sp já esteja definido\n",
        "\n",
        "# Agrupar os dados por 'periodo' e 'centro_expandido'\n",
        "grouped = df_sp.groupby(['periodo', 'centro_expandido'])\n",
        "\n",
        "# Calcular a média de 'meses_duracao' e 'meses_abandono' e a contagem de valores não-nulos\n",
        "result = grouped[['meses_duracao', 'meses_abandono']].agg(['mean', 'count'])\n",
        "\n",
        "# Renomear colunas para maior clareza\n",
        "result.columns = ['tempo_medio_baixa', 'empresas_baixadas', 'tempo_medio_inaptidao', 'empresas_inaptas']\n",
        "\n",
        "# Calcular a contagem total de registros para cada grupo (incluindo valores nulos)\n",
        "total_counts = grouped.size().rename('total_registros')\n",
        "\n",
        "# Combinar os resultados com a contagem total\n",
        "result = pd.concat([result, total_counts], axis=1)\n",
        "\n",
        "# Calcular o percentual de registros\n",
        "result['percentual_baixadas'] = (result['empresas_baixadas'] / result['total_registros']) * 100\n",
        "result['percentual_inaptas'] = (result['empresas_inaptas'] / result['total_registros']) * 100\n",
        "\n",
        "# Arredondar os valores para uma casa decimal\n",
        "result = result.round({'tempo_medio_baixa': 1, 'tempo_medio_inaptidao': 1, 'percentual_baixadas': 1, 'percentual_inaptas': 1})\n",
        "\n",
        "# Resetar o índice para a tabela plotly\n",
        "result = result.reset_index()\n",
        "\n",
        "# Substituir 'N' e 'S' em 'centro_expandido'\n",
        "result['centro_expandido'] = result['centro_expandido'].replace({'N': 'Periferia', 'S': 'Centro Expandido'})\n",
        "\n",
        "# Definir a ordem correta dos períodos\n",
        "periodos_esperados = ['antes', 'pandemia', 'depois']\n",
        "result['periodo'] = pd.Categorical(result['periodo'], categories=periodos_esperados, ordered=True)\n",
        "result = result.sort_values(by=['centro_expandido', 'periodo'])\n",
        "\n",
        "# Definir os cabeçalhos\n",
        "headers = ['Região', 'Período', 'Tempo Médio de Baixa (meses)', 'Empresas Baixadas', 'Tempo Médio de Inaptidão (meses)', 'Empresas Inaptas', 'Total de Registros', 'Percentual de Empresas Baixadas (%)', 'Percentual de Empresas Inaptas (%)']\n",
        "\n",
        "# Preparar os valores das células\n",
        "cells_values = [\n",
        "    result['centro_expandido'].tolist(),\n",
        "    result['periodo'].tolist(),\n",
        "    result['tempo_medio_baixa'].tolist(),\n",
        "    result['empresas_baixadas'].tolist(),\n",
        "    result['tempo_medio_inaptidao'].tolist(),\n",
        "    result['empresas_inaptas'].tolist(),\n",
        "    result['total_registros'].tolist(),\n",
        "    result['percentual_baixadas'].tolist(),\n",
        "    result['percentual_inaptas'].tolist()\n",
        "]\n",
        "\n",
        "# Criar a tabela no Plotly\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(\n",
        "        values=headers,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill_color='lightblue',\n",
        "        font=dict(color='black', size=12),\n",
        "        height=30\n",
        "    ),\n",
        "    cells=dict(\n",
        "        values=cells_values,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill=dict(color=['white', 'lightgray']),\n",
        "        font=dict(color='black', size=11),\n",
        "        height=25\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "RgmRjlflxP9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Supondo que df_sp já esteja definido\n",
        "\n",
        "# Criar a tabela de contagem\n",
        "table = df_sp[['periodo', 'centro_expandido', 'status']].value_counts().reset_index(name='count')\n",
        "\n",
        "# Substituir 'N' e 'S' em 'centro_expandido'\n",
        "table['centro_expandido'] = table['centro_expandido'].replace({'N': 'Periferia', 'S': 'Centro Expandido'})\n",
        "\n",
        "# Definir a ordem correta dos períodos\n",
        "periodos_esperados = ['antes', 'pandemia', 'depois']\n",
        "table['periodo'] = pd.Categorical(table['periodo'], categories=periodos_esperados, ordered=True)\n",
        "\n",
        "# Pivotar a tabela\n",
        "pivot_table = pd.pivot_table(\n",
        "    table,\n",
        "    values='count',\n",
        "    index=['centro_expandido', 'periodo'],\n",
        "    columns=['status'],\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "# Calcular os valores percentuais e arredondar para uma casa decimal\n",
        "pivot_table_percentage = pivot_table.copy()\n",
        "for periodo in periodos_esperados:\n",
        "    for centro in ['Centro Expandido', 'Periferia']:\n",
        "        mask = (pivot_table_percentage['periodo'] == periodo) & (pivot_table_percentage['centro_expandido'] == centro)\n",
        "        total = pivot_table.loc[mask, pivot_table.columns[2:]].sum(axis=1).values[0]\n",
        "        if total > 0:\n",
        "            pivot_table_percentage.loc[mask, pivot_table.columns[2:]] = (pivot_table_percentage.loc[mask, pivot_table.columns[2:]] / total * 100).round(1)\n",
        "\n",
        "# Definir os cabeçalhos\n",
        "headers = ['Região', 'Período', 'Ativo (%)', 'Desenquadrado (%)', 'Empresa Irregular (%)', 'Encerrada (%)']\n",
        "\n",
        "# Preparar os valores das células\n",
        "cells_values = [pivot_table_percentage[col].tolist() for col in pivot_table_percentage.columns]\n",
        "\n",
        "# Criar a tabela no Plotly\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(\n",
        "        values=headers,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill_color='lightblue',\n",
        "        font=dict(color='black', size=12),\n",
        "        height=30\n",
        "    ),\n",
        "    cells=dict(\n",
        "        values=cells_values,\n",
        "        align='center',\n",
        "        line_color='black',\n",
        "        fill=dict(color=['white', 'lightgray']),\n",
        "        font=dict(color='black', size=11),\n",
        "        height=25\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "CTUT5Q10sCkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ramos de atividade"
      ],
      "metadata": {
        "id": "nBtZSPuBKJsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# Carregar os dados do CNAE\n",
        "try:\n",
        "    cnae_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/estudo_mei/cnaes.zip', sep=';', header=None, encoding='latin1', names=['codigo', 'descricao'], compression='zip', dtype={'codigo': str}, on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'cnaes.zip' not found. Please check the file path.\")\n",
        "    cnae_df = pd.DataFrame()  # Create an empty DataFrame if the file isn't found\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: Invalid zip file. Please ensure the file is a valid zip archive.\")\n",
        "    cnae_df = pd.DataFrame()\n",
        "\n",
        "if not cnae_df.empty:\n",
        "    # Garantir que a coluna 'codigo' exista e seja do tipo string.\n",
        "    if 'codigo' not in cnae_df.columns:\n",
        "        print(\"Error: 'codigo' column not found in the provided file.\")\n",
        "    else:\n",
        "        cnae_df['codigo'] = cnae_df['codigo'].astype(str)\n",
        "\n",
        "        # Função para extrair a subclasse\n",
        "        def get_subclass(cnae_code):\n",
        "            try:\n",
        "                return cnae_code[:2]\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "        # Aplicar a função para criar a coluna 'subclass'\n",
        "        cnae_df['subclass'] = cnae_df['codigo'].apply(get_subclass)\n",
        "\n",
        "        # Dicionário de mapeamento das subclasses para as denominações\n",
        "        subclass_mapping = {\n",
        "            '01': 'Agricultura e Pecuária e extrativismo',\n",
        "            '02': 'Agricultura e Pecuária e extrativismo',\n",
        "            '03': 'Agricultura e Pecuária e extrativismo',\n",
        "            '05': 'Agricultura e Pecuária e extrativismo',\n",
        "            '06': 'Agricultura e Pecuária e extrativismo',\n",
        "            '07': 'Agricultura e Pecuária e extrativismo',\n",
        "            '08': 'Agricultura e Pecuária e extrativismo',\n",
        "            '09': 'Agricultura e Pecuária e extrativismo',\n",
        "            '10': 'Processos Fabris',\n",
        "            '11': 'Processos Fabris',\n",
        "            '12': 'Processos Fabris',\n",
        "            '13': 'Processos Fabris',\n",
        "            '14': 'Processos Fabris',\n",
        "            '15': 'Processos Fabris',\n",
        "            '16': 'Processos Fabris',\n",
        "            '17': 'Processos Fabris',\n",
        "            '18': 'Processos Fabris',\n",
        "            '19': 'Processos Fabris',\n",
        "            '20': 'Processos Fabris',\n",
        "            '21': 'Processos Fabris',\n",
        "            '22': 'Processos Fabris',\n",
        "            '23': 'Processos Fabris',\n",
        "            '24': 'Processos Fabris',\n",
        "            '25': 'Processos Fabris',\n",
        "            '26': 'Processos Fabris',\n",
        "            '27': 'Processos Fabris',\n",
        "            '28': 'Processos Fabris',\n",
        "            '29': 'Processos Fabris',\n",
        "            '30': 'Processos Fabris',\n",
        "            '31': 'Processos Fabris',\n",
        "            '32': 'Processos Fabris',\n",
        "            '33': 'Processos Fabris',\n",
        "            '35': 'Eletricidade e Gás',\n",
        "            '36': 'Água e Esgoto',\n",
        "            '37': 'Água e Esgoto',\n",
        "            '38': 'Água e Esgoto',\n",
        "            '39': 'Água e Esgoto',\n",
        "            '41': 'Construção',\n",
        "            '42': 'Construção',\n",
        "            '43': 'Construção',\n",
        "            '45': 'Comércio',\n",
        "            '46': 'Comércio',\n",
        "            '47': 'Comércio',\n",
        "            '49': 'Transporte Terrestre',\n",
        "            '50': 'Transporte Aquaviário',\n",
        "            '51': 'Transporte Aéreo',\n",
        "            '52': 'Logística',\n",
        "            '53': 'Atividades de Entrega',\n",
        "            '55': 'Alojamento',\n",
        "            '56': 'Alimentação',\n",
        "            '58': 'Informação e Comunicação',\n",
        "            '59': 'Produção de Vídeo e Som',\n",
        "            '60': 'Informação e Comunicação',\n",
        "            '61': 'Informação e Comunicação',\n",
        "            '62': 'Serviços de TI',\n",
        "            '63': 'Informação e Comunicação',\n",
        "            '64': 'Atividades Financeiras, de Seguros e Serviços Relacionados',\n",
        "            '65': 'Atividades Financeiras, de Seguros e Serviços Relacionados',\n",
        "            '66': 'Atividades Financeiras, de Seguros 3 Serviços Relacionados',\n",
        "            '68': 'Imobiliárias',\n",
        "            '69': 'Atividades Profissionais, Científicas e Técnicas',\n",
        "            '70': 'Atividades Profissionais, Científicas e Técnicas',\n",
        "            '71': 'Atividades Profissionais, Científicas e Técnicas',\n",
        "            '72': 'Atividades Profissionais, Científicas e Técnicas',\n",
        "            '73': 'Publicidade',\n",
        "            '74': 'Atividades Profissionais, Científicas e Técnicas',\n",
        "            '75': 'Veterinário',\n",
        "            '77': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '78': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '79': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '80': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '81': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '82': 'Atividades Administrativas E Serviços Complementares',\n",
        "            '84': 'Servícos Públicos',\n",
        "            '85': 'Educação',\n",
        "            '86': 'Serviços de Saúde e Sociais',\n",
        "            '87': 'Serviços de Saúde e Sociais',\n",
        "            '88': 'Serviços de Saúde e Sociais',\n",
        "            '90': 'Cultura',\n",
        "            '91': 'Cultura, Esporte e Entretenimento',\n",
        "            '92': 'Jogos de Azar e Apostas',\n",
        "            '93': 'Esporte e Entretenimento',\n",
        "            '94': 'Outros Serviços',\n",
        "            '95': 'Manutenção de Computadores e eletrônicos',\n",
        "            '96': 'Outros Serviços Pessoais',\n",
        "            '97': 'Serviços Domésticos',\n",
        "            '99': 'Órgãos Internacionais'\n",
        "        }\n",
        "\n",
        "\n",
        "        # Mapear as subclasses para as denominações\n",
        "        cnae_df['denominacao'] = cnae_df['subclass'].map(subclass_mapping)\n",
        "\n",
        "        # Exibir o DataFrame resultante\n",
        "        cnae_df = cnae_df[['descricao', 'denominacao']]\n",
        "        print(cnae_df.head())\n",
        "        cnae_df.to_csv('/content/drive/MyDrive/Colab Notebooks/estudo_mei/cnaes_mapeados.zip', index=False, compression='zip')\n"
      ],
      "metadata": {
        "id": "rYKD5b5ZqQKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: criar a coluna 'subclasse_cnae' para o dataframe df_sp onde a partir do das correspondências entre cnae_fiscal_principal de df_sp e descricao de cnae_df retornar denominacao  de cnae_df\n",
        "\n",
        "# Merge the dataframes based on the 'cnae_fiscal_principal' and 'descricao' columns\n",
        "df_sp = pd.merge(df_sp, cnae_df, left_on='cnae_fiscal_principal', right_on='descricao', how='left')\n",
        "\n",
        "# Rename the 'denominacao' column to 'subclasse_cnae'\n",
        "df_sp = df_sp.rename(columns={'denominacao': 'subclasse_cnae'})\n",
        "\n",
        "# Display the first few rows of the updated dataframe to verify the changes\n",
        "print(df_sp[['cnae_fiscal_principal', 'subclasse_cnae']].head())"
      ],
      "metadata": {
        "id": "XpMYNqFxN_8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Plotar um gráfico temporal de áreal com valores acumulados de subclasse_cnae do dataframe df_sp para cadasttros com a situação ativa, com o fundo separado em antes, pandemia e depois.\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Filtrar para estabelecimentos ativos\n",
        "df_ativos_sp = df_sp[df_sp['situacao_cadastral'] == 'ativa']\n",
        "\n",
        "# Agrupar por data e subclasse CNAE, somando os valores\n",
        "df_agrupado_sp = df_ativos_sp.groupby([pd.Grouper(key='data_inicio', freq='ME'), 'subclasse_cnae'])['cnpj_basico'].count().reset_index()\n",
        "df_agrupado_sp = df_agrupado_sp.rename(columns={'cnpj_basico': 'quantidade'})\n",
        "\n",
        "# Calcular a soma acumulada da quantidade de CNPJs por subclasse CNAE\n",
        "df_agrupado_sp['quantidade_acumulada'] = df_agrupado_sp.groupby('subclasse_cnae')['quantidade'].cumsum()\n",
        "\n",
        "# Criar o gráfico de área\n",
        "fig = px.area(df_agrupado_sp,\n",
        "              x='data_inicio',\n",
        "              y='quantidade_acumulada',\n",
        "              color='subclasse_cnae',\n",
        "              title='Valores Acumulados de Subclasse CNAE (Estabelecimentos Ativos)',\n",
        "              labels={'data_inicio': 'Data de Início', 'quantidade_acumulada': 'Quantidade Acumulada', 'subclasse_cnae': 'Subclasse CNAE'})\n",
        "\n",
        "# Adicionar áreas de fundo para as épocas\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WiLMLVcCQT1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Refaça o gráfico da célula anterior trocando a quantidade acumulada de registros por quantidade mensal e a área por linha. Utilize a média móvel de 3 meses para atenuar eventuais sazonalidades.\n",
        "\n",
        "# Calcular a média móvel de 3 meses\n",
        "df_agrupado_sp['quantidade_mm3'] = df_agrupado_sp.groupby('subclasse_cnae')['quantidade'].rolling(window=3, min_periods=1).mean().reset_index(0,drop=True)\n",
        "\n",
        "# Criar o gráfico de linha com a média móvel\n",
        "fig = px.line(df_agrupado_sp,\n",
        "              x='data_inicio',\n",
        "              y='quantidade_mm3',\n",
        "              color='subclasse_cnae',\n",
        "              title='Quantidade Mensal de MEIs por Subclasse CNAE (Média Móvel de 3 Meses)',\n",
        "              labels={'data_inicio': 'Data de Início', 'quantidade_mm3': 'Quantidade (Média Móvel 3 Meses)', 'subclasse_cnae': 'Subclasse CNAE'})\n",
        "\n",
        "# Adicionar áreas de fundo para as épocas\n",
        "fig.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "             fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "             fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "             fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "             annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "tW3v2pwERLVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estudo de bairros"
      ],
      "metadata": {
        "id": "jhUGxURQKcYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roteiro"
      ],
      "metadata": {
        "id": "TfyMCMNhMNsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projeto da pós-graduação\n",
        "\n",
        "- Dados a apurar:\n",
        "\n",
        "1. Crescimento geral\n",
        "  1. Número de cadastros\n",
        "  2. Evolução por situação (acumulado)\n",
        "  3. Mapas (antes, pandemia, depois)\n",
        "\n",
        "2. Crescimento (centro expandido x periferia)\n",
        "  1. Número de cadastros\n",
        "  2. Evolução por situação (acumulado)\n",
        "  3. Tabelas:\n",
        "    1. Quantidade de bairros com mais de cem e mil MEI's ativos, por macrorregião e período;\n",
        "    2. Relação dos dez bairros com mais MEI's criados, por período;\n",
        "    3. Taxa de sobrevivência (% de MEI's baixados), por macrorregião e período;\n",
        "    4. Taxa de inaptidão (% de MEI's inaptos), por macrorregião e período;\n",
        "\n",
        "- Conclusões:\n",
        "\n",
        "1. Houve aumento da capilaridade de MEI's?\n",
        "2. Houve alteração no perfil de ramos de atividades?\n",
        "3. Mudou a resiliência (baixa e inaptidão)?\n",
        "4. Que pontos a identificar e quais propostas e sugestões de melhoria?"
      ],
      "metadata": {
        "id": "RrtEUv9ePP7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento geral - Número de cadastros"
      ],
      "metadata": {
        "id": "og_OnZgwkmR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Faça um gráfico de área para a quantidade acumulada de registros mês a mês de criação do MEI's na cidade de são paulo, ilustrando os períodos pré, pandemia e depois, separando por periferia e centro expandido\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming df_sp is your DataFrame and it has columns:\n",
        "# 'data_inicio' (datetime), 'centro_expandido' (categorical: 'S' or 'N'),\n",
        "# 'quantidade' (numeric - number of MEI registrations)\n",
        "\n",
        "# Create a copy of your DataFrame to avoid modifying the original\n",
        "df_plot = df_sp.copy()\n",
        "\n",
        "# Convert 'data_inicio' to datetime if not already\n",
        "df_plot['data_inicio'] = pd.to_datetime(df_plot['data_inicio'])\n",
        "\n",
        "# Group data and calculate the cumulative sum\n",
        "df_plot = df_plot.groupby(['data_inicio', 'centro_expandido'])['cnpj_basico'].count().reset_index()\n",
        "df_plot = df_plot.rename(columns={'cnpj_basico':'quantidade'})\n",
        "df_plot['quantidade_acumulada'] = df_plot.groupby('centro_expandido')['quantidade'].cumsum()\n",
        "\n",
        "# Create the area chart\n",
        "fig = px.area(df_plot,\n",
        "              x='data_inicio',\n",
        "              y='quantidade_acumulada',\n",
        "              color='centro_expandido',\n",
        "              title='Quantidade Acumulada de Registros de MEIs em São Paulo',\n",
        "              labels={'data_inicio': 'Data de Início',\n",
        "                      'quantidade_acumulada': 'Quantidade Acumulada de MEIs',\n",
        "                      'centro_expandido': 'Região'})\n",
        "\n",
        "fig.for_each_trace(lambda t: t.update(name={'S': 'Centro Expandido', 'N': 'Periferia'}[t.name]))\n",
        "\n",
        "# Add vertical rectangles for the periods\n",
        "fig.add_vrect(x0=pd.Timestamp('2020-02-01'), x1=pd.Timestamp('2022-05-31'),\n",
        "             annotation_text=\"Período da Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"lightgreen\", opacity=0.25, line_width=0)\n",
        "fig.add_vrect(x0=df_plot['data_inicio'].min(), x1=pd.Timestamp('2020-01-31'),\n",
        "              annotation_text=\"Período Pré-Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"lightblue\", opacity=0.25, line_width=0)\n",
        "\n",
        "fig.add_vrect(x0=pd.Timestamp('2022-06-01'), x1=df_plot['data_inicio'].max(),\n",
        "              annotation_text=\"Período Pós-Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"pink\", opacity=0.25, line_width=0)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "F0lq0sqFQOLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_estabelecimentos = restore_dataframe(f'{DIR_DADOS}/estabelecimentos_mei.zip', filtros=f\"municipio_nome == 'SAO PAULO' & uf == 'SP'\")"
      ],
      "metadata": {
        "id": "48HtwSt1gVgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "municipio, uf = 'SAO PAULO', 'SP'\n",
        "locais_sp = logradouros_IBGE(municipio, uf)\n",
        "df_cidade = pd.merge(df_estabelecimentos, locais_sp, on=['logradouro', 'bairro'], how='inner')\n",
        "df_com_coordenadas = df_cidade.dropna(subset=['latitude', 'longitude'])\n",
        "df_enderecos = df_com_coordenadas.groupby('bairro').agg({\n",
        "        'latitude': 'mean',\n",
        "        'longitude': 'mean'\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "wP86QzAsi7uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_enderecos"
      ],
      "metadata": {
        "id": "kNk6_j4_t9Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bairros_contidos = extrair_bairros_kml(f'{DIR_DADOS}/rodizio_sp.kml', df_enderecos)"
      ],
      "metadata": {
        "id": "-YVRKDNyt4xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cidade['centro_expandido'] = 'N'  # Inicializa a coluna com 'N'\n",
        "df_cidade.loc[df_cidade['bairro'].isin(bairros_contidos['bairro']), 'centro_expandido'] = 'S'\n",
        "df_sp_antes = df_cidade[df_cidade['data_inicio'] < '2018-02-03']\n",
        "backup_dataframe(df_sp_antes, f'{DIR_DADOS}/df_sp_antes.zip')"
      ],
      "metadata": {
        "id": "7qypWWkguT4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sp_antes[['centro_expandido','situacao_cadastral']].value_counts()"
      ],
      "metadata": {
        "id": "y-jrbwRSb6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: refaça o gráfico anterior adicionando os valores anteriores apurados anteriores a todos os períodos do gráfico em quantidade acumulada de registros (255898 para N e 84822 para S na coluna centro_expandido)\n",
        "\n",
        "# Assuming df_plot is the DataFrame used for the previous area chart\n",
        "# and it has columns: 'data_inicio', 'centro_expandido', 'quantidade', 'quantidade_acumulada'\n",
        "\n",
        "# Add the initial values for each 'centro_expandido'\n",
        "initial_values = {'S': 84822, 'N': 255898}\n",
        "for centro, value in initial_values.items():\n",
        "    # Create a new row for the initial value\n",
        "    new_row = pd.DataFrame({'data_inicio': df_plot['data_inicio'].min() - pd.Timedelta(days=1),\n",
        "                            'centro_expandido': centro,\n",
        "                            'quantidade': value,\n",
        "                            'quantidade_acumulada': value}, index=[0])\n",
        "    # Concatenate with the existing DataFrame\n",
        "    df_plot = pd.concat([new_row, df_plot], ignore_index=True)\n",
        "\n",
        "# Correct cumulative sum after adding initial values\n",
        "df_plot['quantidade_acumulada'] = df_plot.groupby('centro_expandido')['quantidade'].cumsum()\n",
        "\n",
        "# Create the area chart (using the updated df_plot)\n",
        "fig = px.area(df_plot,\n",
        "              x='data_inicio',\n",
        "              y='quantidade_acumulada',\n",
        "              color='centro_expandido',\n",
        "              title='Quantidade Acumulada de Registros de MEIs em São Paulo por macrorregião',\n",
        "              labels={'data_inicio': 'Data de Início',\n",
        "                      'quantidade_acumulada': 'Quantidade Acumulada de MEIs',\n",
        "                      'centro_expandido': 'Região'})\n",
        "\n",
        "fig.for_each_trace(lambda t: t.update(name={'S': 'Centro Expandido', 'N': 'Periferia'}[t.name]))\n",
        "\n",
        "# Add vertical rectangles for the periods\n",
        "fig.add_vrect(x0=pd.Timestamp('2020-02-01'), x1=pd.Timestamp('2022-05-31'),\n",
        "             annotation_text=\"Período da Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"lightgreen\", opacity=0.25, line_width=0)\n",
        "fig.add_vrect(x0=df_plot['data_inicio'].min(), x1=pd.Timestamp('2020-01-31'),\n",
        "              annotation_text=\"Período Pré-Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"lightblue\", opacity=0.25, line_width=0)\n",
        "\n",
        "fig.add_vrect(x0=pd.Timestamp('2022-06-01'), x1=df_plot['data_inicio'].max(),\n",
        "              annotation_text=\"Período Pós-Pandemia\", annotation_position=\"top left\",\n",
        "             fillcolor=\"pink\", opacity=0.25, line_width=0)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "36oP1DGjhOXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento Geral - Evolução por situação"
      ],
      "metadata": {
        "id": "2PVCzQBYkyEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sp_antes[['centro_expandido','situacao_cadastral']].value_counts()"
      ],
      "metadata": {
        "id": "ghDMFDm4k-IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Converta a coluna 'data_situacao_cadastral' para o tipo datetime\n",
        "df_merged['data_situacao_cadastral'] = pd.to_datetime(df_merged['data_situacao_cadastral'])\n",
        "\n",
        "# Crie um dataframe com todas as combinações possíveis de datas, centro_expandido e situacao_cadastral\n",
        "all_dates = pd.date_range(start=df_merged['data_situacao_cadastral'].min(), end=df_merged['data_situacao_cadastral'].max(), freq='ME')\n",
        "all_combinations = pd.MultiIndex.from_product([all_dates, df_merged['centro_expandido'].unique(), df_merged['situacao_cadastral'].unique()], names=['data_situacao_cadastral', 'centro_expandido', 'situacao_cadastral'])\n",
        "df_all_combinations = pd.DataFrame(index=all_combinations).reset_index()\n",
        "all_combinations_t = pd.MultiIndex.from_product([all_dates, df_merged['situacao_cadastral'].unique()], names=['data_situacao_cadastral', 'situacao_cadastral'])\n",
        "df_all_combinations_t = pd.DataFrame(index=all_combinations_t).reset_index()\n",
        "\n",
        "# Agrupe os dados mensalmente pelo número de registros por cada situação cadastral, divididos pela coluna 'centro_expandido'\n",
        "df_agrupado = df_merged.groupby([pd.Grouper(key='data_situacao_cadastral', freq='ME'), 'centro_expandido', 'situacao_cadastral']).size().reset_index(name='total_registros')\n",
        "df_agrupado_total = df_merged.groupby([pd.Grouper(key='data_situacao_cadastral', freq='ME'),'situacao_cadastral']).size().reset_index(name='total_registros')\n",
        "\n",
        "# Mescle com o dataframe de todas as combinações possíveis para preencher os meses sem dados com zeros\n",
        "df_agrupado = df_all_combinations.merge(df_agrupado, on=['data_situacao_cadastral', 'centro_expandido', 'situacao_cadastral'], how='left').fillna(0)\n",
        "df_agrupado_total = df_all_combinations_t.merge(df_agrupado_total, on=['data_situacao_cadastral', 'situacao_cadastral'], how='left').fillna(0)\n",
        "\n",
        "# Calcule o total acumulado de registros\n",
        "df_agrupado['total_acumulado'] = df_agrupado.groupby(['centro_expandido', 'situacao_cadastral'])['total_registros'].cumsum()\n",
        "df_agrupado_total['total_acumulado'] = df_agrupado_total.groupby(['situacao_cadastral'])['total_registros'].cumsum()\n",
        "\n",
        "df_sp_antes_init = df_sp_antes.groupby(['data_inicio', 'centro_expandido', 'situacao_cadastral']).size().reset_index(name='count')\n",
        "# Atualize o valor inicial para o período anterior contido no dataframe df_sp_antes\n",
        "for index, row in df_sp_antes_init.iterrows():\n",
        "    mask = (df_agrupado['centro_expandido'] == row['centro_expandido']) & (df_agrupado['situacao_cadastral'] == row['situacao_cadastral'])\n",
        "    df_agrupado.loc[mask, 'total_acumulado'] += row['count']\n",
        "\n",
        "# Recalcule o total acumulado de registros para a cidade de São Paulo somando os valores de centro expandido e periferia\n",
        "df_total_sp_corrigido = df_agrupado.groupby(['data_situacao_cadastral', 'situacao_cadastral'])['total_acumulado'].sum().reset_index()\n"
      ],
      "metadata": {
        "id": "8KB7vW0Pyqx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crie um gráfico de área preenchida mensal para o Centro Expandido\n",
        "fig_total_sp_corrigido = px.area(df_total_sp_corrigido,\n",
        "                                 x='data_situacao_cadastral',\n",
        "                                 y='total_acumulado',\n",
        "                                 color='situacao_cadastral',\n",
        "                                 title='Total Acumulado de Situações Cadastrais por Mês (Cidade de São Paulo)',\n",
        "                                 labels={'total_acumulado': 'Total Acumulado', 'data_situacao_cadastral': 'Mês', 'situacao_cadastral': 'Situação Cadastral'})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig_total_sp_corrigido.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "                                 fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                                 annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_total_sp_corrigido.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "                                 fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                                 annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_total_sp_corrigido.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "                                 fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                                 annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "# fig_total_sp_corrigido.add_annotation(\n",
        "#     text='Fonte: Receita Federal, compilado pelo Autor.',\n",
        "#     xref='paper', yref='paper',\n",
        "#     x=0, y=0.05,\n",
        "#     showarrow=False,\n",
        "#     font=dict(size=12)\n",
        "# )\n",
        "\n",
        "fig_total_sp_corrigido.show()\n",
        "\n",
        "# Crie um gráfico de área preenchida mensal para o Centro Expandido\n",
        "df_centro_expandido = df_agrupado[df_agrupado['centro_expandido'] == 'S']\n",
        "fig_centro_expandido = px.area(df_centro_expandido,\n",
        "                               x='data_situacao_cadastral',\n",
        "                               y='total_acumulado',\n",
        "                               color='situacao_cadastral',\n",
        "                               title='Total Acumulado de Situações Cadastrais por Mês (Centro Expandido)',\n",
        "                               labels={'total_acumulado': 'Total Acumulado', 'data_situacao_cadastral': 'Mês', 'situacao_cadastral': 'Situação Cadastral'})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig_centro_expandido.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "                               fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                               annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_centro_expandido.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "                               fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                               annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_centro_expandido.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "                               fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                               annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_centro_expandido.add_annotation(\n",
        "    text='Fonte: Receita Federal, compilado pelo Autor.',\n",
        "    xref='paper', yref='paper',\n",
        "    x=0, y=0.05,\n",
        "    showarrow=False,\n",
        "    font=dict(size=12)\n",
        ")\n",
        "\n",
        "fig_centro_expandido.show()\n",
        "\n",
        "# Crie um gráfico de área preenchida mensal para a Periferia\n",
        "df_periferia = df_agrupado[df_agrupado['centro_expandido'] == 'N']\n",
        "fig_periferia = px.area(df_periferia,\n",
        "                        x='data_situacao_cadastral',\n",
        "                        y='total_acumulado',\n",
        "                        color='situacao_cadastral',\n",
        "                        title='Total Acumulado de Situações Cadastrais por Mês (Periferia)',\n",
        "                        labels={'total_acumulado': 'Total Acumulado', 'data_situacao_cadastral': 'Mês', 'situacao_cadastral': 'Situação Cadastral'})\n",
        "\n",
        "# Adicione uma área de fundo para diferenciar por cor os períodos anterior e posterior a pandemia\n",
        "fig_periferia.add_vrect(x0='2020-02-03', x1='2022-05-22',\n",
        "                        fillcolor=\"lightgreen\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_periferia.add_vrect(x0='2018-02-03', x1='2020-02-02',\n",
        "                        fillcolor=\"lightblue\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pré-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_periferia.add_vrect(x0='2022-05-23', x1='2024-05-22',\n",
        "                        fillcolor=\"pink\", opacity=0.2, layer=\"below\", line_width=0,\n",
        "                        annotation_text=\"Pós-Pandemia\", annotation_position=\"top left\")\n",
        "\n",
        "fig_periferia.add_annotation(\n",
        "    text='Fonte: Receita Federal, compilado pelo Autor.',\n",
        "    xref='paper', yref='paper',\n",
        "    x=0, y=0.05,\n",
        "    showarrow=False,\n",
        "    font=dict(size=12)\n",
        ")\n",
        "\n",
        "fig_periferia.show()"
      ],
      "metadata": {
        "id": "emDMdaYO_CED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento Geral - Mapas e capilaridade"
      ],
      "metadata": {
        "id": "T1KVsd6x0O8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: vamos usar a função plotar_mapa_estabelecimentos(df_merged, 'São Paulo', 'mapa_mei_sp') para gerar três mapas de acordo com os períodos e pré-pandemia, pandemia e pós-pandemia apenas com os registros com situacao_cadastral = ativa\n",
        "\n",
        "# Filtrar o DataFrame para incluir apenas registros com 'situacao_cadastral' igual a 'ativa'\n",
        "df_ativos = df_merged[df_merged['situacao_cadastral'] == 'ativa']\n",
        "\n",
        "# Gerar o mapa para o período pré-pandemia\n",
        "titulo1 = \"Mapa de MEI's Ativos em São Paulo (Pré-Pandemia)\"\n",
        "nota1 = 'Estabelecimentos MEI criados na cidade de São Paulo entre 03/02/2018 e 02/02/2020.'\n",
        "df_pre_pandemia = df_ativos[df_ativos['periodo'] == 'antes']\n",
        "fig_pre_pandemia = plotar_mapa_estabelecimentos(df_pre_pandemia, 'São Paulo', 'mapa_mei_sp_pre_pandemia', titulo=titulo1, nota=nota1)\n",
        "fig_pre_pandemia.show()\n",
        "\n",
        "# Gerar o mapa para o período da pandemia\n",
        "titulo2 = \"Mapa de MEI's Ativos em São Paulo (Pandemia)\"\n",
        "nota2 = 'Estabelecimentos MEI criados na cidade de São Paulo entre 03/02/2020 e 22/05/2022.'\n",
        "df_pandemia = df_ativos[df_ativos['periodo'] == 'pandemia']\n",
        "fig_pandemia = plotar_mapa_estabelecimentos(df_pandemia, 'São Paulo', 'mapa_mei_sp_pandemia', titulo=titulo2, nota=nota2)\n",
        "fig_pandemia.show()\n",
        "\n",
        "# Gerar o mapa para o período pós-pandemia\n",
        "titulo3 = \"Mapa de MEI's Ativos em São Paulo (Pós-Pandemia)\"\n",
        "nota3 = 'Estabelecimentos MEI criados na cidade de São Paulo entre 23/05/2022 e 23/05/2024.'\n",
        "df_pos_pandemia = df_ativos[df_ativos['periodo'] == 'depois']\n",
        "fig_pos_pandemia = plotar_mapa_estabelecimentos(df_pos_pandemia, 'São Paulo', 'mapa_mei_sp_pos_pandemia', titulo=titulo3, nota=nota3)\n",
        "fig_pos_pandemia.show()"
      ],
      "metadata": {
        "id": "QzW0KzGnPQVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento por região - Quantidade de bairros com mais de cem e mil MEI's ativos, por macrorregião e período\n",
        "\n"
      ],
      "metadata": {
        "id": "5kJjifeCBljS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_table(df_sp):\n",
        "    # Agrupa os dados por período, centro expandido e conta a quantidade de bairros com mais de 500 e 1000 MEIs\n",
        "    table_data = df_sp.groupby(['periodo', 'centro_expandido', 'bairro'])['cnpj_basico'].count().reset_index()\n",
        "    table_data = table_data[table_data['cnpj_basico'] > 0]\n",
        "\n",
        "    table_data['mais_de_500_MEIs'] = table_data['cnpj_basico'] > 500\n",
        "    table_data['mais_de_1000_MEIs'] = table_data['cnpj_basico'] > 1000\n",
        "\n",
        "    # Cria a tabela\n",
        "    periodos = ['antes', 'pandemia', 'depois']\n",
        "    centros = ['S', 'N']\n",
        "\n",
        "    header = dict(values=['Período', 'Região', 'Total de Bairros com MEI\\'s', 'Bairros com +500 MEI\\'s', 'Bairros com +1000 MEI\\'s'],\n",
        "                  align='left')\n",
        "\n",
        "    cells_values = [[], [], [], [], []]\n",
        "\n",
        "    for periodo in periodos:\n",
        "        for centro in centros:\n",
        "            cells_values[0].append(periodo)\n",
        "            cells_values[1].append('Centro Expandido' if centro == 'S' else 'Periferia')\n",
        "            total_bairros = table_data[(table_data['periodo'] == periodo) & (table_data['centro_expandido'] == centro)].shape[0]\n",
        "            bairros_500 = table_data[(table_data['periodo'] == periodo) & (table_data['centro_expandido'] == centro) & (table_data['mais_de_500_MEIs'])].shape[0]\n",
        "            bairros_1000 = table_data[(table_data['periodo'] == periodo) & (table_data['centro_expandido'] == centro) & (table_data['mais_de_1000_MEIs'])].shape[0]\n",
        "            cells_values[2].append(total_bairros)\n",
        "            cells_values[3].append(bairros_500)\n",
        "            cells_values[4].append(bairros_1000)\n",
        "\n",
        "    cells = dict(values=cells_values,\n",
        "                 align='left',\n",
        "                 fill_color='paleturquoise',\n",
        "                 font=dict(color='black'))\n",
        "\n",
        "    fig = go.Figure(data=[go.Table(header=header, cells=cells)])\n",
        "\n",
        "    fig.update_layout(\n",
        "    title='Contagem de bairros com MEI\\'s por período e região na cidade de São Paulo',\n",
        "    annotations=[dict(text='Fonte: Receita Federal do Brasil, 2024. Compilado pelo autor.',\n",
        "                      x=0, y=0.5,\n",
        "                      xref='paper', yref='paper',\n",
        "                      showarrow=False,\n",
        "                      xanchor='left')]\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "# Exemplo de uso (substitua 'seu_dataframe.csv' pelo caminho real para seu arquivo)\n",
        "# df_sp = pd.read_csv('seu_dataframe.csv')\n",
        "plot_table(df_sp)\n"
      ],
      "metadata": {
        "id": "9gIaKOBxngMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento por região - Relação dos 10 bairros mais ativos por região"
      ],
      "metadata": {
        "id": "XOqIudMCQDUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_table(df_sp):\n",
        "    # Cria os dados da tabela\n",
        "    antes_top10 = df_sp[df_sp['periodo'] == 'antes'].groupby('bairro')['cnpj_basico'].count().nlargest(10).index.tolist()\n",
        "    pandemia_top10 = df_sp[df_sp['periodo'] == 'pandemia'].groupby('bairro')['cnpj_basico'].count().nlargest(10).index.tolist()\n",
        "    depois_top10 = df_sp[df_sp['periodo'] == 'depois'].groupby('bairro')['cnpj_basico'].count().nlargest(10).index.tolist()\n",
        "\n",
        "    header = dict(values=['Antes da Pandemia', 'Período da Pandemia', 'Após a Pandemia'],\n",
        "                  align='left')\n",
        "\n",
        "    # Determina as cores das células\n",
        "    antes_colors = ['blue' if df_sp[(df_sp['bairro'] == bairro) & (df_sp['periodo'] == 'antes')]['centro_expandido'].iloc[0] == 'S' else 'red' for bairro in antes_top10]\n",
        "    pandemia_colors = ['blue' if df_sp[(df_sp['bairro'] == bairro) & (df_sp['periodo'] == 'pandemia')]['centro_expandido'].iloc[0] == 'S' else 'red' for bairro in pandemia_top10]\n",
        "    depois_colors = ['blue' if df_sp[(df_sp['bairro'] == bairro) & (df_sp['periodo'] == 'depois')]['centro_expandido'].iloc[0] == 'S' else 'red' for bairro in depois_top10]\n",
        "\n",
        "    cells = dict(values=[antes_top10, pandemia_top10, depois_top10],\n",
        "                 align='left',\n",
        "                 font=dict(color=[antes_colors, pandemia_colors, depois_colors]))\n",
        "\n",
        "    fig = go.Figure(data=[go.Table(header=header, cells=cells)])\n",
        "\n",
        "    # Adiciona título e nota de rodapé\n",
        "    fig.update_layout(\n",
        "        title='10 Bairros com mais MEI\\'s ativos São Paulo por Período',\n",
        "        annotations=[dict(text='Fonte: Receita Federal do Brasil, 2024. Compilado pelo autor.',\n",
        "                          x=0, y=0.25,\n",
        "                          xref='paper', yref='paper',\n",
        "                          showarrow=False,\n",
        "                          xanchor='left')]\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# Exemplo de uso\n",
        "# df_sp = pd.read_csv('seu_arquivo.csv')  # Carregue seu dataframe aqui\n",
        "meis_ativos = df_sp[df_sp['situacao_cadastral'] == 'ativa']\n",
        "plot_table(meis_ativos)\n"
      ],
      "metadata": {
        "id": "5WyJCzPvTK0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crescimento por região - Taxa de sobrevivência e inaptidão"
      ],
      "metadata": {
        "id": "mb3X6GljRVIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_survival_and_inaptitude_tables(result):\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    result_copy = result.copy()\n",
        "\n",
        "    # Replace 'S' and 'N' with 'Centro Expandido' and 'Periferia'\n",
        "    result_copy['centro_expandido'] = result_copy['centro_expandido'].replace({'S': 'Centro Expandido', 'N': 'Periferia'})\n",
        "\n",
        "    # Define periods\n",
        "    periods = ['antes', 'pandemia', 'depois']\n",
        "\n",
        "    # Function to create a combined table\n",
        "    def create_combined_table(data, title, duration_col, percent_col):\n",
        "        regions = data['centro_expandido'].unique()\n",
        "        duration_values = [\n",
        "            data[data['periodo'] == period][duration_col].round(1).astype(str).replace('nan', 'Não aplicável').tolist()\n",
        "            for period in periods\n",
        "        ]\n",
        "        percent_values = [\n",
        "            data[data['periodo'] == period][percent_col].round(1).astype(str).replace('nan', 'Não aplicável').tolist()\n",
        "            for period in periods\n",
        "        ]\n",
        "\n",
        "        fig = go.Figure(data=[go.Table(\n",
        "            header=dict(values=['Região'] + [f'Tempo Médio de {title} em Meses ({period})' for period in periods] +\n",
        "                        [f'Percentual de {title} ({period})' for period in periods],\n",
        "                        fill_color='paleturquoise',\n",
        "                        align='left'),\n",
        "            cells=dict(values=[regions] + duration_values + percent_values,\n",
        "                       fill_color='lavender',\n",
        "                       align='left'))\n",
        "        ])\n",
        "        titlefig = f'Tempo médio e percentual de {title} de MEI\\'s na cidade de São Paulo por Região e Período'\n",
        "        fig.update_layout(title=titlefig)\n",
        "        fig.show()\n",
        "\n",
        "    # Combined Survival Rate Table\n",
        "    create_combined_table(result_copy, 'Baixa', 'meses_duracao_mean', 'meses_duracao_percent')\n",
        "\n",
        "    # Combined Inaptitude Rate Table\n",
        "    create_combined_table(result_copy, 'Inaptidão', 'meses_abandono_mean', 'meses_abandono_percent')\n",
        "\n",
        "# Example usage (replace with your actual DataFrame)\n",
        "plot_survival_and_inaptitude_tables(result.reset_index())"
      ],
      "metadata": {
        "id": "5Q61XvWxRgjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de principais atividades"
      ],
      "metadata": {
        "id": "1TNhNluHbyik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_top_cnae_table(df):\n",
        "    # Group data by period, expanded center, and main CNAE code, counting occurrences\n",
        "    cnae_counts = df.groupby(['periodo', 'centro_expandido', 'cnae_fiscal_principal'])['cnpj_basico'].count().reset_index(name='count')\n",
        "\n",
        "    # Sort values within each group to get the top 5 CNAEs\n",
        "    cnae_counts = cnae_counts.sort_values(['periodo', 'centro_expandido', 'count'], ascending=[True, True, False])\n",
        "\n",
        "    # Create the table data\n",
        "    periods = ['antes', 'pandemia', 'depois']\n",
        "    centers = cnae_counts['centro_expandido'].unique()\n",
        "\n",
        "    # Initialize header and cells_values for the table\n",
        "    header = dict(values=['Região'] + periods, align='left')\n",
        "    cells_values = [[], [], [], []]\n",
        "\n",
        "    for center in centers:\n",
        "        # Replace 'N' and 'S' with 'Periferia' and 'Centro Expandido'\n",
        "        region_name = 'Periferia' if center == 'N' else 'Centro Expandido'\n",
        "        cells_values[0].append(region_name)  # Add center to the first column (Região)\n",
        "\n",
        "        for period in periods:\n",
        "            top_cnaes = cnae_counts[(cnae_counts['periodo'] == period) & (cnae_counts['centro_expandido'] == center)].head(5)\n",
        "            cnae_list = top_cnaes['cnae_fiscal_principal'].tolist()\n",
        "            # Ensure there are 5 entries\n",
        "            if len(cnae_list) < 5:\n",
        "                cnae_list.extend([''] * (5 - len(cnae_list)))\n",
        "            # Create a numbered list with HTML line breaks\n",
        "            numbered_cnae_list = [f\"{i+1}. {cnae}<br>\" for i, cnae in enumerate(cnae_list)]\n",
        "            cells_values[periods.index(period) + 1].append(''.join(numbered_cnae_list))\n",
        "\n",
        "    cells = dict(values=cells_values, align='left', fill_color='paleturquoise', height=30)\n",
        "\n",
        "    fig = go.Figure(data=[go.Table(header=header, cells=cells, columnwidth=[40, 200, 200, 200])])\n",
        "\n",
        "    fig.update_layout(title=\"Top 5 CNAE's por Região e Período\")\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "# Exemplo de uso (substitua pelo seu DataFrame)\n",
        "plot_top_cnae_table(df_merged)\n"
      ],
      "metadata": {
        "id": "Nm0P9_z7b6M_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yrRg3R6TX_Ge",
        "741cdBHOZueo",
        "RgyFa8iqZ8x8",
        "1-GtlHEucrkW",
        "-LCarWH_c5_S",
        "nBtZSPuBKJsB"
      ],
      "mount_file_id": "1wjVM7Xd1P22rmOYbeiXvb004fOEST1PH",
      "authorship_tag": "ABX9TyPc2c9+2yt6+BhiCg7TfsD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}